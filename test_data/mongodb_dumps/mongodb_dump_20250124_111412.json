{
  "standards": [
    {
      "_id": "6793749ed353ce01153327bb",
      "text": "# Java Coding Standards\n\n## 1. Naming Conventions\n- Use camelCase for variables, methods, and non-final fields.\n- Use PascalCase for class and interface names.\n- Constants should be in UPPERCASE_WITH_UNDERSCORES.\n- Avoid abbreviations and use meaningful names for readability.\n\n## 2. Indentation and Formatting\n- Use 4 spaces per indentation level; do not use tabs.\n- Place opening braces `{` on the same line as the declaration.\n- Limit line length to 100 characters.\n- Add a single blank line between methods and sections of code.\n\n## 3. File Structure\n- Each class or interface should reside in its own `.java` file.\n- Name files to match the public class or interface they contain.\n- Use package names to organize files into logical groups.\n\n## 4. Comments and Documentation\n- Use Javadoc comments for public classes, methods, and fields.\n- Use block comments (`/* */`) for detailed explanations and inline comments (`//`) sparingly.\n- Document the purpose, parameters, and return values of methods.\n\n## 5. Imports\n- Avoid wildcard imports (e.g., `import java.util.*`).\n- Group imports by:\n  1. Standard library imports\n  2. Third-party library imports\n  3. Project-specific imports\n- Separate each group with a blank line.\n\n## 6. Error Handling\n- Use exceptions for error handling.\n- Catch specific exceptions rather than `Exception` or `Throwable`.\n- Include meaningful messages when throwing or logging exceptions.\n- Use `try`-with-resources for handling closeable resources.\n\n## 7. Code Style\n- Follow the Oracle Code Conventions for the Java Programming Language.\n- Use tools like Checkstyle or SpotBugs to enforce standards.\n- Remove unused variables and imports.\n\n## 8. Testing\n- Use JUnit or TestNG for writing and running tests.\n- Structure tests in a `src/test/java` directory.\n- Name test classes with `Test` suffix, e.g., `UserServiceTest`.\n- Ensure unit tests are independent and repeatable.\n\n## 9. Collections and Streams\n- Use generics to ensure type safety in collections.\n- Prefer `for-each` loops or streams for iteration over collections.\n- Use `Optional` to avoid null pointer exceptions.\n\n## 10. Multithreading\n- Use thread-safe collections like `ConcurrentHashMap` for shared data.\n- Avoid manual thread management; use `ExecutorService` or higher-level abstractions.\n- Always synchronize shared mutable data.\n\n## 11. Performance\n- Avoid creating unnecessary objects.\n- Use StringBuilder for string concatenation in loops.\n- Profile and optimize performance-critical sections of the code.\n\n## 12. Version Control\n- Exclude `.class` and other build artifacts using `.gitignore`.\n- Write meaningful commit messages.\n- Follow a consistent branching strategy (e.g., GitFlow).\n\n## 13. Build and Dependencies\n- Use a build tool like Maven or Gradle for dependency management.\n- Define dependencies explicitly in `pom.xml` or `build.gradle`.\n- Avoid hardcoding version numbers; use a properties file if needed.\n",
      "repository_path": "java_coding_standards.md",
      "standard_set_id": "6793749ca3cf6891cb41f3e0",
      "classification_ids": [
        "67935eda361ba58e4bb051aa"
      ],
      "created_at": "2025-01-24T11:08:14.190000",
      "updated_at": "2025-01-24T11:08:14.190000"
    },
    {
      "_id": "6793749fd353ce01153327bc",
      "text": "# Node.js Coding Standards\n\nThis document outlines the coding standards and best practices for writing Node.js code. Adhering to these standards will ensure consistency, readability, and maintainability across the codebase.\n\n## Table of Contents\n\n1. [General Guidelines](#general-guidelines)\n2. [Code Formatting](#code-formatting)\n3. [Naming Conventions](#naming-conventions)\n4. [Error Handling](#error-handling)\n5. [Asynchronous Code](#asynchronous-code)\n6. [Modules and Imports](#modules-and-imports)\n7. [Testing](#testing)\n8. [Security](#security)\n9. [Documentation](#documentation)\n10. [Version Control](#version-control)\n\n---\n\n## General Guidelines\n\n- **Consistency**: Follow consistent patterns and styles throughout the codebase.\n- **Readability**: Write code that is easy to read and understand. Avoid overly complex logic.\n- **Modularity**: Break down code into small, reusable modules.\n- **Performance**: Optimize for performance, but not at the cost of readability.\n- **Linting**: Use a linter (e.g., ESLint) to enforce coding standards.\n\n---\n\n## Code Formatting\n\n- **Indentation**: Use 2 spaces for indentation.\n- **Semicolons**: Always use semicolons to terminate statements.\n- **Line Length**: Keep lines under 80 characters where possible.\n- **Braces**: Use braces `{}` for all control structures, even single-line blocks.\n- **Quotes**: Use single quotes (`'`) for strings, unless escaping is necessary.\n- **Trailing Commas**: Use trailing commas in multi-line object and array literals.\n\n```javascript\n// Good\nconst obj = {\n  name: 'John',\n  age: 30,\n};\n\n// Bad\nconst obj = {\n  name: 'John',\n  age: 30\n};\n```\n\n---\n\n## Naming Conventions\n\n- **Variables**: Use `camelCase` for variable names.\n- **Constants**: Use `UPPER_SNAKE_CASE` for constants.\n- **Functions**: Use `camelCase` for function names.\n- **Classes**: Use `PascalCase` for class names.\n- **Files**: Use `kebab-case` for file names.\n\n```javascript\n// Good\nconst userName = 'John';\nconst MAX_USERS = 100;\n\nfunction getUserData() {}\nclass UserProfile {}\n```\n\n---\n\n## Error Handling\n\n- **Throw Errors**: Always throw `Error` objects, not strings or other types.\n- **Try/Catch**: Use `try/catch` blocks for synchronous error handling.\n- **Promises**: Handle errors in promises using `.catch()` or `async/await` with `try/catch`.\n- **Error Logging**: Log errors using a logging library (e.g., `winston` or `bunyan`).\n\n```javascript\n// Good\ntry {\n  someSyncOperation();\n} catch (error) {\n  logger.error('Error occurred:', error);\n}\n\n// Good\nsomeAsyncOperation()\n  .then(() => {})\n  .catch((error) => {\n    logger.error('Error occurred:', error);\n  });\n```\n\n---\n\n## Asynchronous Code\n\n- **Promises**: Prefer using promises over callbacks.\n- **Async/Await**: Use `async/await` for better readability and error handling.\n- **Callback Hell**: Avoid deeply nested callbacks (callback hell) by using promises or `async/await`.\n\n```javascript\n// Good\nasync function fetchData() {\n  try {\n    const data = await someAsyncOperation();\n    return data;\n  } catch (error) {\n    logger.error('Error fetching data:', error);\n  }\n}\n```\n\n---\n\n## Modules and Imports\n\n- **CommonJS**: Use `require` and `module.exports` for CommonJS modules.\n- **ES Modules**: Use `import` and `export` if using ES modules.\n- **Module Organization**: Organize modules by functionality, and avoid creating monolithic files.\n\n```javascript\n// CommonJS\nconst fs = require('fs');\nmodule.exports = { someFunction };\n\n// ES Modules\nimport fs from 'fs';\nexport { someFunction };\n```\n\n---\n\n## Testing\n\n- **Test Coverage**: Aim for high test coverage, but focus on meaningful tests.\n- **Testing Frameworks**: Use `Mocha`, `Jest`, or similar testing frameworks.\n- **Assertions**: Use `Chai` or `Jest` assertions for writing tests.\n- **Mocking**: Use `Sinon` or `Jest` for mocking dependencies.\n\n```javascript\n// Example using Mocha and Chai\ndescribe('User Module', () => {\n  it('should return user data', async () => {\n    const user = await getUserData();\n    expect(user).to.have.property('name');\n  });\n});\n```\n\n---\n\n## Security\n\n- **Input Validation**: Always validate and sanitize user input.\n- **Dependencies**: Regularly update dependencies to avoid vulnerabilities.\n- **Environment Variables**: Use environment variables for sensitive data (e.g., API keys).\n- **HTTPS**: Always use HTTPS for production APIs.\n\n```javascript\n// Example using environment variables\nconst apiKey = process.env.API_KEY;\n```\n\n---\n\n## Documentation\n\n- **JSDoc**: Use JSDoc for documenting functions, classes, and modules.\n- **README**: Include a `README.md` file in each module or project.\n- **Inline Comments**: Use inline comments sparingly, only when necessary to explain complex logic.\n\n```javascript\n/**\n * Fetches user data from the API.\n * @param {string} userId - The ID of the user.\n * @returns {Promise<Object>} - The user data.\n */\nasync function fetchUserData(userId) {\n  // Implementation\n}\n```\n\n---\n\n## Version Control\n\n- **Commit Messages**: Write clear and descriptive commit messages.\n- **Branching**: Use feature branches and follow a Git workflow (e.g., Git Flow).\n- **Code Reviews**: Conduct code reviews to ensure quality and adherence to standards.\n\n```bash\n# Good commit message\ngit commit -m \"feat: add user authentication module\"\n```\n\n---\n\nBy following these standards, you will contribute to a clean, maintainable, and efficient Node.js codebase. Happy coding! \ud83d\ude80\n",
      "repository_path": "nodejs_coding_standards.md",
      "standard_set_id": "6793749ca3cf6891cb41f3e0",
      "classification_ids": [
        "6790cb8170024c99a94725a5",
        "678fcbe9c24985192f15edc2"
      ],
      "created_at": "2025-01-24T11:08:15.285000",
      "updated_at": "2025-01-24T11:08:15.285000"
    },
    {
      "_id": "679374a1d353ce01153327bd",
      "text": "\n\nChatGPT 4o\nYou said:\ngenerate a set of coding standards in .md format, one file for each standard.  some standards should be applicable to any programming language.  then one for python, one for .net, one for java\nChatGPT said:\nChatGPT\nJava Coding Standards\nI have created a set of coding standards in markdown format:\n\nGeneral Coding Standards: Applicable to all programming languages.\nPython Coding Standards: Specific guidelines for Python.\n.NET Coding Standards: Tailored for .NET development.\nJava Coding Standards: Targeted at Java programming.\nLet me know if you'd like to expand or refine any of these documents!\n\n\n\n\n\n\nYou said:\nwrite the general coding standards doc\nChatGPT said:\nChatGPT\nEdited Java Coding Standards\nThe General Coding Standards document has been written. Let me know if you'd like further edits or refinements!\n\n\n\n\n\n\nYou said:\nwrite the python standards\nChatGPT said:\nChatGPT\nEdited Java Coding Standards\nThe document has been updated with Python-specific coding standards. Let me know if you need further edits or additional details!\n\n\n\n\n\n\nYou said:\nwrite the .net coding standards\nChatGPT said:\nChatGPT\nEdited Java Coding Standards\nThe document has been updated with .NET-specific coding standards. Let me know if there are any other adjustments or additional content you'd like to include!\n\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT can make mistakes. Check important info.\n?\n\nJava Coding Standards\n\n\n\n\n\nTA\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n# .NET Coding Standards\n\n## 1. Naming Conventions\n- Use PascalCase for class, method, and property names.\n- Use camelCase for local variables and parameters.\n- Prefix interfaces with an `I` (e.g., `IService`).\n- Use meaningful and descriptive names; avoid abbreviations.\n\n## 2. Code Layout\n- Use 4 spaces per indentation level.\n- Place braces `{}` on a new line for classes, methods, and properties.\n- Use a single blank line to separate methods and logical code blocks.\n\n## 3. File Organization\n- Place one class, interface, or struct per file.\n- Name the file after the class or interface it contains.\n- Organize files into folders based on functionality or feature.\n\n## 4. Error Handling\n- Use `try`...`catch` blocks to handle exceptions, and log meaningful messages.\n- Avoid catching `System.Exception`; catch specific exceptions instead.\n- Use `using` statements for resources that need disposal.\n\n## 5. LINQ\n- Prefer LINQ over loops for readability when dealing with collections.\n- Use query syntax or method syntax consistently within a project.\n\n## 6. Async/Await\n- Use `async` and `await` for asynchronous operations.\n- Suffix async methods with `Async` (e.g., `GetDataAsync`).\n- Avoid blocking asynchronous code by using `.Result` or `.Wait()`.\n\n## 7. Testing\n- Use a testing framework like MSTest, NUnit, or xUnit.\n- Organize tests in separate projects or directories.\n- Name test methods clearly, e.g., `MethodName_StateUnderTest_ExpectedBehavior`.\n\n## 8. Documentation\n- Use XML comments to document public methods, properties, and classes.\n- Include summary, parameters, and return details in the documentation.\n- Enable XML documentation generation in the project settings.\n\n## 9. Code Style\n- Use tools like `StyleCop` or `ReSharper` to enforce coding standards.\n- Follow the official .NET coding conventions for consistency.\n\n## 10. Dependency Injection\n- Use dependency injection to manage dependencies and improve testability.\n- Prefer constructor injection over property injection.\n\n## 11. Configuration\n- Store configuration in appsettings.json or environment variables.\n- Use strongly-typed configuration classes for better maintainability.\n\n## 12. Version Control\n- Exclude `bin/`, `obj/`, and other temporary files using `.gitignore`.\n- Use descriptive commit messages and branch names.\n- Follow GitFlow or a similar branching strategy.\nChatGPT says: The document has been updated with .NET-specific coding standards. Let me know if there are any other adjustments or additional content you'd like to include!\n",
      "repository_path": "dotnet_coding_standards.md",
      "standard_set_id": "6793749ca3cf6891cb41f3e0",
      "classification_ids": [
        "678fcc01c24985192f15edc6",
        "678fcc2bc24985192f15edcc"
      ],
      "created_at": "2025-01-24T11:08:17.103000",
      "updated_at": "2025-01-24T11:08:17.103000"
    },
    {
      "_id": "679374a2d353ce01153327be",
      "text": "# JavaScript Coding Standards\n\n## 1. **Code Structure**\n- Use meaningful and descriptive names for variables, functions, and classes.\n- Organize code into reusable modules or components.\n- Follow the single responsibility principle (SRP) for functions and classes.\n\n## 2. **Syntax and Formatting**\n- Use camelCase for variable and function names, and PascalCase for class names.\n- Use `const` for constants and `let` for variables that may change; avoid `var`.\n- Prefer template literals (`\\``) over string concatenation for better readability.\n- Use strict equality (`===`) to avoid type coercion.\n\n## 3. **Code Style**\n- Indent using 2 spaces or a tab (be consistent across the project).\n- End statements with semicolons (`;`) for clarity and to avoid ambiguity.\n- Use single quotes (`'`) for strings unless double quotes are required.\n\n## 4. **Error Handling**\n- Use `try...catch` blocks for error-prone operations.\n- Validate inputs and handle edge cases gracefully.\n- Log meaningful error messages for debugging.\n\n## 5. **Comments and Documentation**\n- Write comments to explain the \"why\" of the code, not the \"what.\"\n- Use JSDoc-style comments for functions and complex blocks.\n- Keep comments up-to-date with code changes.\n\n## 6. **Best Practices**\n- Avoid global variables; use closures or modules instead.\n- Write unit tests for critical functions and components.\n- Use linting tools (e.g., ESLint) to enforce coding standards.\n- Minimize the use of inline JavaScript in HTML for better separation of concerns.\n\n## 7. **Version Control**\n- Commit changes with descriptive messages.\n- Follow a branching strategy (e.g., Git Flow) for version control.\n\nFollowing these guidelines ensures that your JavaScript code is maintainable, readable, and scalable.\n",
      "repository_path": "javascript_standards.md",
      "standard_set_id": "6793749ca3cf6891cb41f3e0",
      "classification_ids": [
        "678fcbe9c24985192f15edc2"
      ],
      "created_at": "2025-01-24T11:08:18.199000",
      "updated_at": "2025-01-24T11:08:18.199000"
    },
    {
      "_id": "679374a3d353ce01153327bf",
      "text": "# General Coding Standards\n\n## 1. Naming Conventions\n- Use meaningful and descriptive names for variables, functions, and classes.\n- Use camelCase for variables and functions.\n- Use PascalCase for class names.\n- Constants should be written in UPPERCASE_WITH_UNDERSCORES.\n\n## 2. Code Organization\n- Group related functions and variables together.\n- Separate code into modules or files based on functionality.\n- Use comments to document the purpose of each module or file.\n\n## 3. Indentation and Formatting\n- Use consistent indentation (e.g., 4 spaces per level).\n- Limit line length to 80-100 characters.\n- Add a newline at the end of each file.\n\n## 4. Documentation\n- Write clear and concise comments explaining why the code exists, not just what it does.\n- Use docstrings or comments to describe the purpose and usage of functions and classes.\n\n## 5. Error Handling\n- Validate inputs and handle errors gracefully.\n- Use exceptions for error handling where supported by the language.\n- Log errors with sufficient context to aid debugging.\n\n## 6. Version Control\n- Use version control (e.g., Git) for all projects.\n- Commit changes with meaningful and descriptive messages.\n- Use feature branches for new work and merge to main via pull requests.\n\n## 7. Testing\n- Write unit tests for critical functions and components.\n- Aim for high test coverage to ensure code reliability.\n- Use automated testing tools where possible.\n\n## 8. Security\n- Avoid hardcoding sensitive data like passwords or API keys.\n- Use secure practices for handling user input to prevent vulnerabilities like SQL injection or XSS.\n- Regularly update dependencies to patch security vulnerabilities.\n\n## 9. Code Reviews\n- Review code for readability, maintainability, and correctness before merging.\n- Provide constructive feedback during code reviews.\n- Address feedback promptly and thoroughly.\n\n## 10. Performance\n- Optimize code for efficiency when necessary, but prioritize readability and maintainability first.\n- Use profiling tools to identify and resolve bottlenecks.\n- Avoid premature optimization unless performance is critical.\n",
      "repository_path": "general_coding_standards.md",
      "standard_set_id": "6793749ca3cf6891cb41f3e0",
      "classification_ids": [],
      "created_at": "2025-01-24T11:08:19.154000",
      "updated_at": "2025-01-24T11:08:19.154000"
    },
    {
      "_id": "679374a4d353ce01153327c0",
      "text": "# Python Coding Standards\n\n## 1. Naming Conventions\n- Follow PEP 8 guidelines for naming:\n  - Variables and functions: snake_case\n  - Classes: PascalCase\n  - Constants: UPPERCASE_WITH_UNDERSCORES\n- Avoid single-character variable names except for counters or iterators (e.g., `i`, `j`).\n\n## 2. Code Layout\n- Use 4 spaces per indentation level.\n- Limit lines to 79 characters.\n- Surround top-level function and class definitions with two blank lines.\n- Use one blank line to separate methods within a class.\n\n## 3. Imports\n- Use absolute imports whenever possible.\n- Group imports in the following order:\n  1. Standard library imports\n  2. Related third-party imports\n  3. Local application/library imports\n- Separate each group with a blank line.\n\n## 4. Strings\n- Use single quotes (`'`) or double quotes (`\"`) consistently within a project.\n- Prefer f-strings for formatting over `format()` or `%`.\n\n## 5. Exceptions\n- Use specific exception classes rather than a generic `Exception`.\n- Always include relevant information in exception messages.\n- Use `try`...`except` blocks sparingly and avoid nesting them deeply.\n\n## 6. Typing\n- Use type hints for function arguments and return values.\n```python\nfrom typing import List\n\ndef add_numbers(a: int, b: int) -> int:\n    return a + b\n```\n\n## 7. Documentation\n- Write docstrings for all public modules, classes, and functions.\n- Follow the Google or NumPy style for docstrings.\n\n## 8. Testing\n- Use `pytest` or `unittest` for writing and running tests.\n- Structure tests in separate directories, e.g., `tests/`.\n- Name test files starting with `test_`.\n\n## 9. Code Style\n- Use tools like `flake8` or `pylint` to enforce coding standards.\n- Use `black` for consistent code formatting.\n- Avoid using wildcard imports (e.g., `from module import *`).\n\n## 10. Performance\n- Use built-in functions and libraries whenever possible (e.g., `sum()`, `len()`).\n- Avoid unnecessary list comprehensions when a generator is sufficient.\n\n## 11. Virtual Environments\n- Use `venv` or `virtualenv` for managing dependencies.\n- Include a `requirements.txt` or `pyproject.toml` file for dependency tracking.\n\n## 12. Version Control\n- Exclude `.pyc` and other temporary files using `.gitignore`.\n- Commit changes with meaningful messages, describing the purpose of the change.\n",
      "repository_path": "python_coding_standards.md",
      "standard_set_id": "6793749ca3cf6891cb41f3e0",
      "classification_ids": [
        "678fcbf5c24985192f15edc4"
      ],
      "created_at": "2025-01-24T11:08:20.124000",
      "updated_at": "2025-01-24T11:08:20.124000"
    },
    {
      "_id": "67937586dd8933f95f8d8ef8",
      "text": "# GitHub access\n\nThis guide covers covers all things related to accessing and using [GitHub](https://github.com) in the context of working on a Defra project.\n\n## GitHub account requirements\n\nBefore you can do anything on GitHub you'll need to [create an account](https://github.com/join). If you already have one it is okay to use it, else create a new one specifically for your Defra work if you prefer.\n\nWhether you create an account or use an existing one, it must meet the following requirements\n\n- [Two factor authentication](https://help.github.com/articles/about-two-factor-authentication/) must be enabled\n- Your profile must have the `Name` field populated with your full name .e.g. *Alan Cruikshanks*.\n\nYou're not required to set a profile picture, but changing it from the default GitHub provides will help your team members distinguish your contributions from theirs.\n\n## Joining a Defra organisation\n\nAll projects at Defra must be created under one of the organisations within the Defra GitHub enterprise:\n\n- [Defra](https://github.com/DEFRA) for development of digital services\n- [Defra Data Science Centre of Excellence](https://github.com/-Defra-Data-Science-Centre-of-Excellence) for data science\n- [Defra design team](https://github.com/defra-design) for prototype designs\n- [aphascience](https://github.com/aphascience) for scientific projects at APHA\n\nThe organisation owners can be contacted to add you to their organisation:\n\n- for digital service development or design, use the #github-support channel on the Defra Digital Slack workspace\n- for data science visit the [Internal Defra DASH SharePoint site](https://defra.sharepoint.com/sites/Community448) for contact details\n- for scientific projects at APHA visit the [Internal Defra SCE SharePoint site](https://defra.sharepoint.com/teams/Team741) for contact details\n\nDepending on your role and requirements, you may either be added as a member of the organisation or as an [outside collaborator](https://help.github.com/articles/adding-outside-collaborators-to-repositories-in-your-organization/) on individual repositories.\n\n## Getting a new repository\n\nYou should contact the organisation owners to create a new repository. They'll need\n\n- A name for the repository - this should be consistent with the existing organisational naming conventions\n- Whether the repository is to be private or public - and if it is private the [justification](https://www.gov.uk/government/publications/open-source-guidance/when-code-should-be-open-or-closed) for this\n- The GitHub username of the person that will be its administrator\n- (Optional) A description - having a short description helps other people quickly identify what is in your repository\n\n**Do not create repositories under your own user account!** Though repositories can be transferred at a later date, it is easier for everyone if they originate within our organisations.\n\n## Administering a repository\n\nIf you are the administrator for a repository it's your responsibility to ensure the repo has been set up and maintained in accordance with the standards of the organisation.\n\nDepending on the organisation settings, you may be able to create and maintain teams to help with administering who has access to your repositories. If not, you may still be able to request the creation of teams from your organisations owners.\n\nHowever you choose to manage it, you must always ensure that your repository's **Collaborators and teams** setting accurately reflects who should have access to the repository.\n\nIf you will no longer be the administrator for a repository, you will need to identify a replacement and make them the administrator.\n\nRepositories without an administrator will be archived by the organisation owners.\n\n## Access removal\n\nThe administrators of our Defra GitHub enterprise will run quarterly reports of [dormant users](https://docs.github.com/en/enterprise-cloud@latest/admin/managing-accounts-and-repositories/managing-users-in-your-enterprise/managing-dormant-users) and provide these to the owners of all the organisations in the enterprise.\n\nThe owners will have a week to respond with the details of any dormant users that should not be removed, but after that time any other accounts will be removed from the enterprise. Organisation owners will need to remove any dormant outside collaborators.\n\nThis will ensure that we are using our licences effectively and that we are applying appropriate security controls on our information.\n\nIf you need access again after your account is removed, you will need to ask to join an organisation again as described above.\n\nIf you think you may be identified as a dormant user, for example if you are taking an extended break from work, then you should let an organisation owner know and they will ensure that your account is not removed.\n\n## Getting a new organisation\n\nIf you would like to have a new organisation added into the Defra enterprise you should contact one of the enterprise administrators to discuss your requirements.\n\n## Owning an organisation\n\nAs an organisation owner you are responsible for\n\n- setting the standards and processes for access control in the organisation\n- checking whether access is required for dormant users and removing dormant outside collaborators from your organisation\n- ensuring that people that will want to join your organisation know who you are and how to contact you\n",
      "repository_path": "docs/processes/github_access.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:06.106000",
      "updated_at": "2025-01-24T11:12:06.106000"
    },
    {
      "_id": "67937588dd8933f95f8d8ef9",
      "text": "# Credential exposure\n\nCredentials are things like passwords or API keys. They are any values you wouldn't want exposed to the public but which your app or service needs to do its job.\n\nWe haven't settled on a specific standard for how we handle them e.g. [12 factor app](https://12factor.net/config). But the one thing we are all agreed on is that they should not be committed with your project's source code.\n\nYet mistakes happen and sometimes these secrets become exposed.\n\nThis process will take you through what to do if that happens. The key point is\n\n> once out *always* consider your secret as compromised\n\nNo matter the speed with which you rectify the situation, or how little you think the chance is for people to see it. Treat it as compromised.\n\n## Contact operations\n\nThe first step is to make contact with the team responsible for your production environment.\n\nFor example if your service is hosted in AWS, you should call someone from the AWS web-ops team ([David Blackburn](https://github.com/davidblackburn) or [Tom Tant](https://github.com/TTEA1990)).\n\nWe cannot post contact numbers here. But they are available using our internal systems.\n\nFollow this up with an email that contains a link to the commit, and explains what was exposed.\n\n## Fix your git\n\nNext is fix your git tree. This involves removing the offending commit from your git history.\n\n### On a branch\n\nIf the commit was on a branch the fix involves amending or removing the commit. You can use git commands such as `git commit --amend` or `git rebase -i` to amend the current branches history.\n\nYou will then follow this with a `git push -f`.\n\n### On master\n\nIf it was the last commit on master, you can get away a `git commit --amend` and a `git push -f` if you have the rights.\n\nElse you will need to follow a much more involved strategy. GitHub provide a [guide](https://help.github.com/en/github/authenticating-to-github/removing-sensitive-data-from-a-repository). If you are unsure contact the development community or speak to our principal developers.\n\n## Remove caches\n\nTo quote GitHub's guide\n\n> [..] *it's important to note that those commits may still be accessible in any clones or forks of your repository, directly via their SHA-1 hashes in cached views on GitHub, and through any pull requests that reference them.*\n\nIf someone manages to fork or update their fork before you push your fix there is little you can do. Try to reach out to them to explain the situtation, and ask them to update their fork.\n\n### GitHub\n\nYou can get the cache of your commit removed by contacting [GitHub support](https://support.github.com/contact). As long as what you need removed meets GitHub's [criteria for sensitive data](https://help.github.com/en/github/site-policy/github-sensitive-data-removal-policy#what-is-sensitive-data) there will not be a problem.\n\nThe [Ruby services team](https://github.com/DEFRA/ruby-services-team) report a turn around of less than an hour when they made this request.\n\nInform the relevant operations team once the request is processed.\n",
      "repository_path": "docs/processes/credential_exposure.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:08.735000",
      "updated_at": "2025-01-24T11:12:08.735000"
    },
    {
      "_id": "6793758bdd8933f95f8d8efa",
      "text": "# Pull requests\n\nThe following outlines how to make changes to a repo. The goals are\n\n- Focused PR's related to specific changes\n- Code review is simple and manageable\n- A clean tree of changes in the commit history\n- Consistency across the commits\n\nWe also want to respect the time and effort of those reviewing the work. Keeping PR's small, focused and consistent makes their life easier.\n\n## Always on a branch\n\nNo matter how small the change, all changes should be done on a branch and never directly to `master`. This is to support the principle of [Master is always shippable](https://github.com/DEFRA/software-development-standards/blob/master/principles/coding_principles.md#master-is-always-shippable)\n\nClone the repo then create your new branch. For example `git checkout -b add-ea-admin-area-lookup`.\n\n## Start with a commit\n\nStart your new branch with an empty commit.\n\n```bash\ngit commit --allow-empty\n```\n\nThe template for the commit is the following\n\n```text\n50 character limited title\n\nLink to originating story/bug/card in relevant system (e.g. Jira or Trello)\n\nDescription covering why we're making this change, and briefly what the change is.\n```\n\nFor example\n\n```text\nHandle empty params in ValidatePreUpdate method\n\nhttps://eaflood.atlassian.net/browse/WAS-1096\n\nThis change fixes an issue found with `validate_pre_update_organisation_address` in that when passed empty parameters it would cause an `undefined method all? for nil:NilClass` error to be thrown.\n```\n\nThe key point is that it should cover **the actual change you are intending to make**, and not just repeat **what the backlog story outlined**. The link to the story is added for those that need that greater context.\n\n### Commit message rules\n\n1. Separate subject from body with a blank line\n1. Limit the subject line to 50 characters\n1. Capitalize the subject line\n1. Do not end the subject line with a period\n1. Use the imperative mood in the subject line (*Add ability* not *Added ability*)\n1. Use the body to explain *what and why* vs. *how*\n\nN.B. The source for these is [How to write a Git commit message](http://chris.beams.io/posts/git-commit/) which you're encouraged to read.\n\n## Let everyone know\n\nNext push your branch.\n\n```bash\ngit push -u origin add-ea-admin-area-lookup\n```\n\nWe create a PR right from the start so that your proposed change is visible to all. This means everyone can feedback if they see any issues, and give help and advice.\n\nOnce pushed you'll need to go to GitHub to actually create the PR. Typically it will highlight the new branch in the UI and provide a quick option to create the PR. You'll then see GitHub automatically takes your first empty commit and uses it to populate the pull request title and description. Nice! Assign yourself to the PR so everyone knows the work is with you.\n\n## Now code\n\nNow get on and code. You should commit frequently and push often. Don't worry too much about your commit messages here. But don't ignore them completely as they will be used as part of the PR process (so keep `WIP`, `Still WIP`, and `More WIP!` to a minimum thank you!)\n\n## Keep up to date\n\nIt's on you to keep your branch up to date with with your main development branch e.g. *master*. Using `rebase` rather than `merge` will mean no merge messages appearing in your PR's commit history, but its not required. To `rebase` on *master* use\n\n```bash\ngit rebase origin/master\n```\n\n## Get it looked at\n\nWhen you're finished and have pushed your last commit request someone to review it. If there are multiple members on your team and all could review, feel free to request them all. The key thing is at least one other person should review the PR before it is merged.\n\nYou and the reviewer will then work to confirm the changes are OK. Once the reviewer is happy they need to **approve it**.\n\n## Completing the PR\n\nOnce approved to complete the PR you'll need to `squash` your commits down to one.\n\nThe simplest way to do this is in the GitHub UI. Within the PR the merge button for new repo's will say `Squash and merge`. Else you may need to click the down arrow next to the button to select it.\n\n<img src=\"squash_and_merge.png\" alt=\"Squash and merge\" style=\"width: 384px;\"/>\n\nWhen you click it GitHub will present a box which contains the combined text from all the commit messages in the PR. Use this opportunity to reword the content to a single commit message (the smaller the change the simpler this is to do!)\n\nWhen done ensure you delete the branch. Again GitHub will present this option in the UI immediately after merging so make use of it then.\n",
      "repository_path": "docs/processes/pull_requests.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:11.213000",
      "updated_at": "2025-01-24T11:12:11.213000"
    },
    {
      "_id": "6793758ddd8933f95f8d8efb",
      "text": "# Kubernetes guidance\n\nThis guidance is aimed at those deploying to and supporting Kubernetes clusters and includes helpful how to guides for common Kubernetes tasks and lessons learned.\n\n## Configure NGINX Ingress Controller\nAn [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) is an application that runs in a Kubernetes cluster and configures an HTTP load balancer according to Ingress resources.\n\nIn the case of NGINX, the Ingress controller is deployed in a pod along with\nthe load balancer.\n\n### Installation\nThe documentation for [NGINX's chart](https://github.com/kubernetes/ingress-nginx) includes details on how to install it.\n\n*TL;DR:*\n\n```\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm install my-release ingress-nginx/ingress-nginx\n```\n\n### Creating a namespace for a service\nEach service in a cluster will have their own dedicated namespace in each cluster.\n\nThis allows logical separation between services as well as the enabling simpler implementation of monitoring, RBAC and stability mechanisms.\n\n## Requirements\nThe following are the suggested outcomes of a namespace.\n- `ResourceQuota` resource to limit resource usage\n- `RoleBinding` resource to restrict interaction to delivery team\n\nExample setup instructions are included in the [FFC Kubernetes Configuration](https://github.com/DEFRA/ffc-kubernetes-configuration) repository.\n\n## Install Kubernetes dashboard\nThe Kubernetes dashboard is a web-based Kubernetes user interface.\n\n### Installation\nInstall the dashboard to a Kubernetes cluster using the `kubectl apply` command specified in the **Deploying the dashboard UI** section in the below link.\n\nhttps://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/\n\nExample:\n`kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml`\n\n#### Create default user and access token\nFollow the guide in the below link.\nhttps://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md\n\n#### Run dashboard\n1. Run terminal command `kubectl proxy`\n1. Access dashboard at http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n\n## Interact with cluster\n`kubectl` is used to interact with the cluster.  In order to use `kubectl` with an FFC cluster, a `kubeconfig` file for the cluster is required.\n\n### Acquiring a Kubeconfig file for a cluster in AKS\nTo acquire a Kubeconfig, a subscription Id is needed along with the name of the cluster and the resource group in which it resides.  This information can be acquired via the Azure Portal or from CSC.\n\n`az account set --subscription SUBSCRIPTION_ID`\n\n`az aks get-credentials --resource-group RESOURCE_GROUP --name CLUSTER --file WHERE_TO_SAVE_KUBECONFIG`\n\n**Warning** if the file parameter is not passed, the Kubeconfig will be merged with the users default configuration file stored at `~/.kube/config`.\nThis can result in the file becoming corrupted due to the nature of the merge.\n\n## Productivity Tools\nDevelopers may find it more productive to use tools such as [k9s](https://github.com/derailed/k9s) or [kubectl-aliases](https://github.com/ahmetb/kubectl-aliases) to avoid needing to regularly type long terminal commands to interact with the cluster.\n\n## Authenticating with cloud resources\nBoth Azure and AWS' managed Kubernetes service, AKS and EKS respectively offer mechanisms to authenticate with cloud resources without the need for credential management in applications.\n\n### AAD Pod Identity (Azure)\nWith this approach, Pod Identity pods are deployed to a cluster.\nApplication code then uses these resources to request tokens to use in subsequent authentication requests to Azure resources.  Identity mappings are also deployed to the cluster.\n\nToken lifecycle is managed within the application itself, some Azure resources have official SDKs that manage this lifecycle, but others the developer must handle the token refresh themselves.\n\nSee full setup details in the [official documentation](https://github.com/Azure/aad-pod-identity).\n\n### IAM role for Service Accounts (AWS)\nWith this approach, only an identity mapping resource is required and the cluster will automatically ensure a valid token is available to the application.\n\nAs token lifecycle is managed outside of the application, this approach results in less complex application code than the Azure implementation.\n\nSee full setup details in the [official documentation](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)\n\n### Azure - AAD Pod Identity\nAAD Pod Identity enables Kubernetes applications to access cloud resources securely with Azure Active Directory (AAD).\n\nUsing Kubernetes primitives, administrators configure identities and bindings to match pods. Then your containerized applications can leverage any resource in the cloud that depends on AAD as an identity provider.\n\nSee Kubernetes [Secret standards](../standards/kubernetes_standards.md) for standards for secret management when Pod Identity is not suitable.\n\n## Probes\nKubernetes has two types of probes, readiness and liveness.\n\nKubernetes uses readiness probes to know when a container is ready to start accepting traffic.\n\nKubernetes uses liveness probes to know when to restart a container.\n\n### Configuring probes\nProbes can be configured in the Helm chart on a `Deployment` resource, under the container node.\n\nThe below is a simple example of an HTTP readiness and liveness probes.\n\n```\nreadinessProbe:\n  path: /healthy\n  port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  failureThreshold: 3\n\nlivenessProbe:\n  path: /healthz\n  port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  failureThreshold: 3\n```\n\nIn this example, the cluster will wait for 10 seconds after the pod is deployed.  It will then poll both the liveness and readiness endpoints on port 3000 every 10 seconds.  \n\nIf it receives three successive status codes other than 200 for the readiness probe it will stop routing traffic to that pod.\n\nIf it receives three successive status codes other than 200 for the liveness probe it will assume the pod is unresponsive and kill it.\n\n**Note** that a liveness probe works in conjunction with the restartPolicy value. In order to restart the restartPolicy must be set to Always or OnFailure.\n\n#### Values\n`path`: the URL route the liveness probe should sent a response to\n\n`port`: the port on which the service is exposing\n\n`initialDelaySeconds`: how long before the first probe should be sent. This should be safely longer than it takes the pod to start up, otherwise the pod could be stuck in a reboot loop\n\n`periodSeconds`: how often the liveness probe should check the pod is responsive. Recommendation is between 10 and 20 seconds\n\n`failureThreshold`: how many probe failures before the pod is automatically restarted\n\nAs well as HTTP probes, there are also command and TCP based probes, full details can be found in the documentation https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n\n## Descheduler\nScheduling in Kubernetes is the process of binding pending pods to nodes, and is performed by a component of Kubernetes called kube-scheduler. \n\nThe scheduler's decisions, whether or where a pod can or can not be scheduled, are guided by its configurable policy which comprises of set of rules, called predicates and priorities. \n\nThe scheduler's decisions are influenced by its view of a Kubernetes cluster at that point of time when a new pod appears for scheduling. \n\nAs Kubernetes clusters are very dynamic and their state changes over time, there may be desire to move already running pods to some other nodes for various reasons:\n\n- some nodes are under or over utilised\n- the original scheduling decision does not hold true any more, as taints or labels are added to or removed from nodes, pod/node affinity requirements are not satisfied any more\n- some nodes failed and their pods moved to other nodes\n- new nodes are added to clusters\n- consequently, there might be several pods scheduled on less desired nodes in a cluster\n\nDescheduler, based on its policy, finds pods that can be moved and evicts them\n\nDescheduler can be installed using Helm as described [here](https://github.com/kubernetes-sigs/descheduler#install-using-helm)\n\n## Autoscaling\nKubernetes pods and nodes can be automatically scaled on demand.  See below resources for usage.\n\n[Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n\n[Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)\n\n[Cluster autoscaling (AKS)](https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler)\n\n[Cluster autoscaling (EKS)](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/)\n",
      "repository_path": "docs/guides/kubernetes.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:13.130000",
      "updated_at": "2025-01-24T11:12:13.130000"
    },
    {
      "_id": "6793758edd8933f95f8d8efc",
      "text": "# Java Auto-format with Eclipse\n\n[Eclipse](https://en.wikipedia.org/wiki/Eclipse_(software)) is an\n\n> integrated development environment (IDE) used in computer programming. It contains a base workspace and an extensible plug-in system for customizing the environment. Eclipse is written mostly in Java and its primary use is for developing Java applications, but it may also be used to develop applications in other programming languages via plug-ins.\n\nWe have a [standard for Java](../standards/java_coding_standards.md) which can be applied automatically when saving a file in Eclipse.\n\n## The format file\n\nIn these guides is a 'code style' `.xml` that can be used called [defra_java_eclipse_code_style.xml](defra_java_eclipse_code_style.xml).\n\n## Importing the file\n\nIn Eclipse, select `Windows > Preferences > Java > Code Style > Formatter > Import`.\n\nSelect the file and then apply the change.\n\n## Applying the format on save\n\nIn Eclipse, select `Windows > Preferences > Java > Editor > Save Actions` and then check\n\n- `Perform the selected actions on save`\n- `Format source code`\n",
      "repository_path": "docs/guides/java_auto_format_eclipse.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [
        "67935eda361ba58e4bb051aa"
      ],
      "created_at": "2025-01-24T11:12:14.902000",
      "updated_at": "2025-01-24T11:12:14.902000"
    },
    {
      "_id": "67937590dd8933f95f8d8efd",
      "text": "# Session Manager\n\nThe purpose of this guide is demonstrate how to use AWS Session Manager to gain access EC2 instances, instead of using SSH.\n\n## Prerequisites\n\n- You will need an AWS account and your user must be part of the relevant developer group/s.\n- You must either have access to the AWS console, or have installed the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) and [Session Manager plugin](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html). You will then need to [configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html). \n\nIf you need any assistance, don't hesitate to contact the CCoE AWS WebOps team.\n\n## User permissions\nWhen initiating a session, you will be logging into the server as the `developer` user. This user has limited permissions and is intended for basic investigative work only. You won't be able to:\n- Run `sudo`, `pm2`, `docker` among other administrative commands.\n- Switch to other users.\n- Copy files onto or off the server.\n\nIf there is anything that you can't do, but that you need to do, please contact the CCoE AWS WebOps team to discuss further. \n\n\n## Using the AWS CLI\n### Starting a session\nFind which instances are available for you to connect to:\n```\naws ec2 describe-instances --query \"Reservations[*].Instances[*].{Instance:InstanceId,Name:Tags[?Key=='Name']|[0].Value,Status:State.Name}\" --output table\n```\n\nCopy the instance ID and then start a session into the instance:\n```\naws ssm start-session --target <instance-id>\n```\n\nYou can find [other examples of starting sessions here](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html), which includes port forwarding.\n\nIf you are having difficulty connecting, please see the [troubleshooting guide here](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-troubleshooting.html). If you're still having issues, please contact <ddts-aws-webops@defra.gov.uk> for support.\n\n### Ending a session\nTypically you will be connecting to an Ubuntu server, you can just execute the `exit` command to come out of the session.\n\nHowever, if your session is stuck, you first need to find your session ID:\n```\naws ssm describe-sessions --state Active\n```\n\nCopy the session ID and then terminate your session with this:\n```\naws ssm terminate-session --session-id <session-id>\n```\n\n## Using the AWS Console\n\n1. Login to the [AWS Console](https://aws.amazon.com/console/)\n2. Navigate to *EC2 dashboard -> instances*\n3. Select a **single** instance that you want to connect to\n4. Select *Connect* and then *Session Manager*\n    - If you are presented with `We weren't able to connect to your instance`, please contact the CCoE AWS WebOps team.",
      "repository_path": "docs/guides/aws_session_manager.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:16.709000",
      "updated_at": "2025-01-24T11:12:16.709000"
    },
    {
      "_id": "67937591dd8933f95f8d8efe",
      "text": "# Mono repo and a multi repo\n\nA mono repo is a single repository that contains all the code for multiple projects. A multi repo is a collection of repositories, each containing the code for a single project. Both approaches have their own advantages and disadvantages. This guide will help you decide which approach is best for your project.\n\nTeams should proactively choose between a mono repo and a multi repo based on the project requirements. Multi repositories are the preferred approach with mono repositories only being used for small single project cases.\n\nThe decision should be made at the beginning of the project, as it can be difficult to switch between the two later on.  The level of difficultly will typically depend on the duration of time the project has been running.\n\nThe decision should be reviewed periodically to ensure that it still meets the project requirements. If the project requirements change, it may be necessary to switch between a mono repo and a multi repo.\n\n## Considerations\n\nTeams should consider the following key drivers for choosing between a mono repo and a multi repo:\n\n- Size and complexity of the project\n- Technology choices\n- Application composition\n- Collaboration\n- CI/CD\n- Versioning and release process\n- Commit history\n- Security\n- Ownership\n\n### Size and complexity\n\nA mono repo is best suited for small, single language projects with a low number of dependencies. A multi repo is best suited for large, complex projects with multiple languages and dependencies.\n\nLarger projects can be difficult to manage in a mono repo, as the size and complexity of the codebase can make it difficult to navigate and understand. In this case, a multi repo may be a better choice, as it allows teams to work on smaller, more manageable codebases.\n\nSize of downloading the repository is also a consideration. A mono repo can be large and take a long time to download, particularly for new team members. A multi repo can be smaller and quicker to download, as each repository contains a smaller amount of code.\n\nIn all cases teams should consider the content of their repositories to ensure that they are not storing unnecessary files or data.\n\nPerformance of scanning tools such as secret detection tools is also increased with smaller repositories.\n\n### Technology choices\n\nMono repos are best suited for projects that use a single technology stack. Multi repos are best suited for projects that use multiple technology stacks.\n\nIf a project uses multiple technology stacks, then it is more difficult to keep a single repository structure consistent.\n\nMultiple technologies in a single repository can also lead to conflicts between any plugins or dependencies that are used by the different technologies within the repository.\n\n### Application composition\n\nMono repos are best suited if components are compiled into a single application.  However, all other considerations should be taken into account before making a decision as even with monolithic application, a multi repo may be the better approach.\n\nMulti repos are best suited if components are independently deployable applications.\n\n### Collaboration\n\nMono repos simplify collaboration as all the code is in one place.  This reduces the complexity of access control and visibility of code, pull requests, issues and release notes.\n\nHowever mono repos may also tightly coupled collaboration needs across independent teams.  This can lead to conflicts and bottlenecks in the development process.\n\nMulti repos are more difficult to collaborate as data is distributed across multiple repositories.  This can mean key data is missed which leads to risk.\n\nIf multiple teams support different components then a mono repository should not be used.  Similarly if individuals within a team are responsible for different components such as frontend and backend, then a mono repository should not be used.\n\n### Versioning and release process\n\nFor simple projects a single version may be sufficient within a mono repo, however for more complex projects a versioning strategy should be considered.  This may involve additional tooling to independently version and release projects.\n\nTools such as [Lerna](https://lerna.js.org/) can help manage dependencies and build processes but typically only support one technology stack.\n\nMulti repos can have independent versioning strategies for each project which can simplify the release process.\n\nIf not composed into a single application, independent components should be released and versioned independently.\n\nThe scale of the service should be considered as releasing or rolling back a large scale mono repo code base can add unnecessary risk and complexity.\n\n### CI/CD\n\nMono repos simplify CI/CD as multiple projects can be built and tested together. This can reduce the complexity of creating and managing multiple pipelines and dependencies.\n\nHowever, mono repos can also lead to longer build times and increased complexity in managing dependencies, particularly if multiple technology choices are used.\n\nAdditional tooling and complexity may be required to independently build and test projects within a mono repo depending on the scale.\n\nMulti repos require more configuration for CI/CD as each project needs its own pipeline which can lead to duplication of effort and complexity in co-ordination of pipeline updates.\n\nThe advantage is a more optimised pipeline for each project.\n\n### Commit history\n\nMono repos have a single commit history which can make it easier to track changes and understand the history of the codebase.\n\nMulti repos have multiple commit histories which can make it more difficult to track changes and understand the history of the codebase, however the history is more focused.\n\n\n### Security\n\nMono repos can simplify security as access control and visibility is centralised.  Dependency scanning tools are also simpler to maintain.\n\nMono repos can also increase the risk of a single point of failure and increase the impact of a security incident.\n\nIt may also be difficult to restrict access to sensitive code or data within a mono repo.\n\nMulti repos can increase the complexity of security as access control and visibility is distributed across multiple repositories increasing the risk stale users may have access to sensitive data.\n\nMulti repos allow for more granular access control.\n\n### Ownership\n\nMono repos work when a single team owns all the code and is expected to maintain it.\n\nMulti repos support multiple teams owning different parts of the codebase.\n\n## Conclusion\n\nAs per [Defra version control standards](../standards/version_control_standards.md), teams should follow a multi repository approach as the default.  Mono repositories should only be used for small scale projects consisting of a single application or package with only one development team.\n",
      "repository_path": "docs/guides/mono_or_multi_repo.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:17.991000",
      "updated_at": "2025-01-24T11:12:17.991000"
    },
    {
      "_id": "67937593dd8933f95f8d8eff",
      "text": "# Guide for new Defra developers\n\nWelcome to Defra! This guide is an overview of tools and resources available to you as a developer, and how to get access to them.\n\n## Developing in government\n\n### Defra development standards\n\nYou can find [our software development standards](https://github.com/DEFRA/software-development-standards/) on GitHub (in this very repository!)\n\n### Cross-government standards\n\nThe Government Digital Service (GDS) sets standards for government digital services.\n\nYou can read these standards here:\n\n- [Service standard](https://www.gov.uk/service-manual/service-standard)\n- [Service manual](https://www.gov.uk/service-manual)\n\n## Meet the community\n\n### Get onto Slack\n\nDigital teams in Defra use Slack to keep in touch. Slack is a messaging app that lets you talk to colleagues in your team, department, or even across government.\n\n[Join the Defra Digital Slack](https://defra-digital.slack.com/)\n\nIf you have a Defra email address, you should be able to sign up to this Slack workspace yourself.\n\nYou may also want to join the [UK Government Digital Slack](https://ukgovernmentdigital.slack.com/). This has thousands of members from across government, and useful channels about software development and security.\n\n### Come to a developer meetup\n\nDeveloper community of practice meetups are a chance to meet other developers, share what you\u2019re working on, and find out news and updates about best practice.\n\nParticipating in the community of practice is considered part of your core job, so we encourage both permanent and contract devs to come along.\n\nMeetups usually happen in our four hub offices, but developers from any office are welcome to attend.\n\n#### North East meetup\n\n- Meets every other Thursday at 2pm in Foss House, York\n- Organised by John Watson\n- Check the #york-dev-test-cop channel in [defra-digital Slack](https://defra-digital.slack.com/) for updates\n\n#### North West meetup\n\n- Meets every Monday at 1pm in Richard Fairclough House, Warrington\n- Organised by Paul Shaw and Lynne Farrow\n\n#### South West meetup\n\n- Meets every other Monday at 2pm in Temple Quay House, Bristol\n- Organised by Alan Cruikshanks\n- Check the #development channel in [defra-digital Slack](https://defra-digital.slack.com/) for updates\n\n## Tech and tools\n\nWhat technology you\u2019ll be using depends on the service you work on. Here are some common resources and tools to be aware of, and how to access them.\n\n### Access to Git, VPN, Jenkins and more\n\nAccess to GitHub, GitLab, Jenkins and other infrastructure services is managed by the Web Ops team.\n\nWe aim to develop in the open on GitHub. However, some repos (especially things like environment config) are privately stored on GitLab.\n\nYou will need VPN access to get to GitLab, Jenkins and our development environments.\n\nContact the Web Ops team or another developer to request access to these services.\n\n### Access to Jira\n\nMany teams use the project management tool Jira to manage their work.\n\n[How to request Jira access](https://github.com/DEFRA/qa-test/wiki/Jira)\n",
      "repository_path": "docs/guides/new_starters.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:19.080000",
      "updated_at": "2025-01-24T11:12:19.080000"
    },
    {
      "_id": "67937594dd8933f95f8d8f00",
      "text": "# Version control guidance\n\nThis guidance helps to explain how to apply version control to our code.\n\nIt is compliant with our version control standards, but provides some additional information on ways to implement version control practice.\n\n## Teams\n\nTeams should follow a multi repository approach and should avoid putting everything related to a project in a single repository.\n\nFor example, put each component (application, package, library, etc) into its own repository, and use [GitHub teams](https://help.github.com/en/articles/organizing-members-into-teams) to manage access to them.\n\nThis makes it easy to apply version control to each component individually, whilst also ensuring that access to the repositories is simple to control.\n\nA mono repository should only be considered for small scale projects consisting of a single application or package.\n\nFor additional context behind this standard, see the [guidance](../guides/mono_or_multi_repo.md).\n",
      "repository_path": "docs/guides/version_control_guidance.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:20.182000",
      "updated_at": "2025-01-24T11:12:20.182000"
    },
    {
      "_id": "67937596dd8933f95f8d8f01",
      "text": "# Continuous integration\n\nNOTE: This guidance only applies to repositories and projects which are public on GitHub and can therefore take advantage of several free integrations. We intend to expand upon this guide in the future to cover the rest of our ecosystem.\n\n> Continuous Integration (CI) is a development practice that requires developers to integrate code into a shared repository several times a day. Each check-in is then verified by an automated build, allowing teams to detect problems early.\n>\n> [ThoughtWorks](https://www.thoughtworks.com/continuous-integration)\n\nThis automated build can check several things, including:\n\n- the application compiles successfully (if it needs compiling)\n- unit tests pass\n- code style checks pass (for example, linters)\n\n## CI with GitHub\n\nMany services can integrate with GitHub and automatically respond to new commits or pull requests. So every time you push code to GitHub, it will trigger the build and report the status.\n\n[GitHub Actions](https://github.com/features/actions/) is free to use for open source GitHub repositories.\n\nYou can configure your repository to always require certain checks when merging to protected branches (like `master`). If a required check fails, it will block merging until the problem is fixed.\n\nWe recommend making the build a required check. This should prevent anyone from merging breaking changes into the master branch.\n\nThere are lots of other tools which can integrate with GitHub, especially if your repository is open source.\n\n### Security\n\nThese tools check the security of your project. This can include reporting vulnerabilities in your dependencies, or doing static analysis on your code.\n\n- Use [Snyk](https://snyk.io/) for Node.js projects\n- Use [Hakiri](https://hakiri.io/) for Ruby projects\n\nThese tools are free to use for open source GitHub repositories.\n\n### Maintainability and test coverage\n\nDefra has a [SonarCloud](https://www.sonarsource.com/products/sonarcloud/) organisation, which should be used to perform static quality analysis checks on your code.\nIt provides a rating for the security, reliability and maintainability of your code and estimates the time it would take to deal with any technical debt.\n\nYou should include SonarCloud in your CI so that it flags problems it spots in your code, like duplication or complexity.\n\nYou can also configure your build tool (like GitHub Actions) to report unit test coverage to SonarCloud.\nSonarCloud will then include your test coverage in its assessment of your code.\n\nSonarCloud is free to use for open source GitHub repositories.\n\n## CI with Jenkins\n\nWe use Jenkins for our internal build servers.\n",
      "repository_path": "docs/guides/continuous_integration.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:22.180000",
      "updated_at": "2025-01-24T11:12:22.180000"
    },
    {
      "_id": "67937597dd8933f95f8d8f02",
      "text": "# PL/SQL Auto-format with TOAD\n\n[Toad](https://en.wikipedia.org/wiki/Toad_(software)) is a\n\n> [..] database management toolset from Quest Software that database developers, database administrators, and data analysts use to manage both relational and non-relational databases using SQL.\n\nSome teams in Defra have licenses for Toad, and use it when working with our Oracle databases.\n\nWe have a [standard for PL/SQL](../standards/plsql_coding_standards.md) which can be applied automatically using the auto-format tool in Toad.\n\n## The format file\n\nIn these guides is an auto-format `.opt` file that can be used called [defra_plsql_toad_fmt.opt](defra_plsql_toad_fmt.opt).\n\n## Importing the file\n\nIn Toad, open an Editor window. Right click on the editor pane and select `Formatting Tools > Formatting Options`.\n\nSelect the file open icon at the top left of the screen (\"Load options from file\" is the tool tip) and then browse to where you have located a copy of the file. Select the file then hit \"Apply\". Then hit \"OK\" to close the formatter options window.\n\n## Applying the format\n\nTo apply the formatter options to any SQL or PL/SQL code open the code in the Editor Window, right click and then select `Formatting Tools > Format`.\n",
      "repository_path": "docs/guides/plsql_auto_format_toad.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:23.130000",
      "updated_at": "2025-01-24T11:12:23.130000"
    },
    {
      "_id": "67937598dd8933f95f8d8f03",
      "text": "# Developer Workflows\n\nWhen working with Git most teams use one of two different workflows depending on their preference.\n\n- [Feature branch workflow](https://www.atlassian.com/git/tutorials/comparing-workflows/feature-branch-workflow) (also known as [GitHub flow](https://guides.github.com/introduction/flow/))\n- [Gitflow](https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow)\n\nThough FBW is the simpler of the two, it should be used by teams that are confident whenever they change the **master** branch it still remains ready for production ([Master is always shippable](https://github.com/DEFRA/software-development-standards/blob/master/principles/coding_principles.md#master-is-always-shippable)).\n\nGitflow is useful for teams that like the space to combine multiple features, or are working on a large feature, before then shipping to production.\n\n## Feature branch workflow (GitHub flow)\n\n<img src=\"githubflow.png\" alt=\"GitHub flow\" style=\"width: 600px;\"/>\n<sub><https://guides.github.com/activities/hello-world/branching.png></sub>\n\nUsing this workflow we only use two branches\n\n- **Master** is the main branch developers work from. It represents the latest version of the code. The key principle is that master is always production ready. Anything merged in needs to have been peer reviewed, passed by CI, and ready for release.\n\n- A **Feature** branch is started each time we want to add to, update or fix something in the code. We branch off Master when creating the feature, and once complete merge it back in\n\n## All branches\n\nIn all cases when the branch is created it should be pushed up to the **origin** repo and a pull request created as per the [pull request](../processes/pull_requests.md) process.\n\n## Gitflow\n\n<img src=\"gitflow.png\" alt=\"Git flow\" style=\"width: 600px;\"/>\n<sub><https://www.atlassian.com/git/images/tutorials/collaborating/comparing-workflows/gitflow-workflow/05.svg></sub>\n\nUsing this workflow it means our branches have specific uses\n\n- **Master** is the version of code that is in production (see the principle [Master is always shippable](https://github.com/DEFRA/software-development-standards/blob/master/principles/coding_principles.md#master-is-always-shippable))\n\n- We create a **Hotfix** branch when we need to make a change to production code because of a critical error. When finished we merge the change back into Master, but also Develop\n\n- **Develop** is the main branch the developers work from. It represents the current version of the code, including new features we\u2019ve completed but not yet released. You should make this the [default branch](https://help.github.com/en/articles/setting-the-default-branch) in GitHub\n\n- A **Feature** branch is started each time we want to add to, update or fix something in the code. We branch off Develop when creating the feature, and once complete merge it back into Develop\n\n- Once we have a set of features we want to put live, we create a **Release** branch. Last minute fixes and tidying up is done on this branch and then it is merged into Master (put live) and back into Develop\n\n### Use of tools\n\nThere are a number of tools you can use to help you with gitflow, for example tools such as [Sourcetree](https://www.sourcetreeapp.com/) have support built in, and you can add support to git [via extensions](https://github.com/nvie/gitflow).\n\nHowever these all assume that merging will be done locally and then pushed to the origin repo. Because we create pull requests on all the branches we create, and merge them using the GitHub web UI it means we cannot use these tools for merging.\n\nSo feel free to use them for creating your branches, however you may find it easier to simply manually create your branches and just ensure you stick to gitflow's naming convention.\n",
      "repository_path": "docs/guides/developer_workflows.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:24.044000",
      "updated_at": "2025-01-24T11:12:24.044000"
    },
    {
      "_id": "6793759add8933f95f8d8f04",
      "text": "# Using Unmanaged Devices\n\nA guide for everyone that needs to use unmanaged devices.\n\n### What is this about?\nThis guide is about the appropriate steps you should take when using devices that are not managed by the normal\norganisational IT controls.\n\nThat means \"off-network\" laptops used, for example, by developers, testers and designers on digital services.\n\n### Does it apply to you?\nMost people are issued with devices have been pre-configured with the organisation's standard IT controls in place.\n\nFor some people, however, these standard devices are too restricted and prevent us from performing our duties.\n\nIn these cases, you will have been issued with an _unmanaged_ device.\n\n#### How do I know if my device is unmanaged?\nYou will have been issued with the device by your line manager and informed that it is an unmanaged device.\n\nIf you have a device and you do not know whether it is managed or unmanaged then ask your line manager.\n\n#### What if it is my own device?\nWe do not allow people to use their own devices for official work. Please don't do it.\n\n> This includes personal smartphones chatting on [Slack](https://slack.com), personal tablets used to\n> [appear.in](https://appear.in/), etc.  Seriously, please don't.\n\nOne day we might have thought through the policy for official data on privately-owned equipment sufficiently to make\nthis happen, but we're not there yet.\n\n#### How do I know if this guidance applies for my device?\nThis guidance applies for _all_ unmanaged devices.\n\n### Who can you talk to about this?\nIf you have any questions then please contact your line manager.\n\nFor suggestions feel free to create an Issue or raise a PR with your suggested changes.\n\n### What is different for unmanaged devices?\nThe main difference is that **you must apply all the necessary security controls to the device yourself**.\n\nIf you are not comfortable with the full set of responsibilities outlined in this guide then speak to your line manager.\nIt may be best to return the unmanaged device and replace it with a managed device.\n\n### What are the security controls you need to apply?\nThe precise controls that you need to apply will vary depending on the type of device that you have and how you need\nto use it.\n\nHowever, there are some common controls that you must apply in all cases and these are described first.\n\n### Common controls\nThese controls must be applied whatever type of unmanaged device you use.\n\n#### Be aware of your responsibilities\nFamiliarise yourself with your organisation's information handling responsibilities, acceptable use policies and code\nof conduct for the use of IT and digital resources.\n\nWe need to be especially mindful of these as the standard IT security controls often help us to comply with them\nwithout us even realising it.\n\n#### Understand the security principles\nRead the [CESG End User Devices Security Guidance](https://www.ncsc.gov.uk/guidance/end-user-device-security), in\nparticular the [Security Principles](https://www.ncsc.gov.uk/guidance/end-user-devices-security-principles).\n\nThis should give you a good understanding of the kind of controls that are required and why they are necessary.\n\nIf you have any questions then speak to your line manager.\n\n#### Ensure your device is registered on the appropriate inventory\nYour line manager must have a record of your device and it must be recorded in a shared inventory.  In most cases, the\ndevice itself will also have an asset identifier on it, usually in the form of a sticker.\n\nThis ensures that we can keep track of the device and prevent it becoming lost when people move roles.  It also allows\nto audit equipment for security compliance.\n\n#### Keep your device patched and up to date\nWhatever device you are using, you will not be receiving updates distributed by your organisation so you will need to\napply these yourself.\n\nYou should normally apply all patches and updates immediately.\nThe only exception to this is where you need to maintain older versions of software to do compatibility testing.\n\n#### Ensure that you are using official online resources\nOne of the main reasons for having unmanaged devices is to be able to make use of online resources.\nHowever, this freedom of access can also be a threat that may cause devices to be compromised or leak official\ninformation.\n\nFor this reason, only officially approved online resources should be accessed from unmanaged devices.\n\nIn reality, any resource that you access for legitimate work is likely to be \"officially approved\", but you are\nresponsible for all of your online activity and will be accountable for any incidents resulting from inappropriate\naccess.\n\nIf you are not sure whether an online resource is considered to be officially approved then speak to your line manager.\n\n#### Use public networks securely\nYou should be aware whenever you are connecting to open, public networks as these can represent an information\nsecurity risk.\nIn particular, the captive portals typically found in cafes and hotels that require a username and password may not\nprovide secure end-to-end encryption.\n\nWhenever you are connecting to a public network, you must ensure that you first evaluate whether it is safe to do so.\n\n#### Do not compromise the organisation's corporate network\nUnder normal circumstances you must never connect your device to the organisation's internal corporate network.\n\nHowever, for some work it may be appropriate to use the device on the corporate network and when this has been\nofficially approved then you must ensure that the network is not compromised.\n\n#### Keep the device physically secure\nUnmanaged devices are generally portable and often highly valuable, so you must take appropriate steps to keep the\ndevice physically secure.\n\nIn particular, you should:\n - keep the device hidden whenever it is being transported or stored\n - ensure the device is stored in a secure, locked location\n - ensure the device is adequately protected from damage at all times\n\nCases, bags or other containers can be provided by your line manager.\n\n#### Use the device safely\nUnmanaged devices often have different physical characteristics than standard-issue equipment.\n\nYou must make sure that you complete an assessment of how to use the device in a way that does not cause strain or\nlong-term injury.\n\nSome specific areas to look out for are:\n - screen size, graphics resolution and text size may not be optimal in the default configuration\n - on-screen or built-in keyboards, touchpads or trackpads are often not suitable for long periods of use\n - default operating system or software configuration can often not be suitable for long periods of use\n - devices may not have the necessary connectivity options for external equipment\n\nAdditional equipment or guidance can be provided by your line manager.\n\n#### Fix any faults\nIf your device develops any hardware faults it could represent a safety or security issue.\n\nThese devices are not managed by the normal corporate IT support functions so you should contact your line manager\nimmediately to have any faults fixed.\n\nIt may be possible to get certain repairs done under warranty, which could involve contacting the support desk or our\nequipment suppliers, but even so you should still speak to your line manager first.\n\n#### Backup and secure data\nUsing unmanaged devices often requires the creation of data outside of our secure, corporate repositories.\n\nYou must ensure that any unique content you create is backed up to alternative persistent storage in accordance with\nthe information management procedures that apply.\n\nAdditional equipment such as encrypted hard drives or online services such as OneDrive can be provided by your\nline manager.\n\n#### Returning devices\nWhen you return your device, you must ensure that it has been treated to remove all official information and is\nready to be used by someone else.\n\nThis usually just means performing a factory reset, although in some cases additional steps may be required.\nYou must ensure that there are no online accounts linked to the device, such as Microsoft or Apple IDs.\n\n### Controls for Windows laptops\n\n#### Don't set a BIOS password\nBIOS passwords don't add any greater security to the device and primarily only help to prevent accidental\nmis-configuration.\n\nSetting such passwords can make it more difficult to reuse the device so do not set a BIOS password.\n\n#### Use the latest stable desktop operating system\nThe device is likely to come with a suitable version of Windows 10 pre-installed, but if not then you should ensure\nthat Windows 10 Professional or Enterprise is installed on it, but do not remove any existing recovery partition.\n\nGuidance on this can be provided by your line manager.\n\nYou must ensure that the device has the latest stable version.\n\nDo not use server versions or different operating systems, e.g. any Linux distribution.\n\nIf you need to work with a different operating system then you must use a virtualisation technology,\nor you could consider using the Windows Subsystem for Linux.\n\n#### Keep the operating system and applications patched and up to date\nWindows 10 quality updates must be applied automatically and not paused.\n\nFeature updates to new versions should be applied as soon as they are available for your device, but may be\ndeferred for a month or two if you have specific software compatibility issues.\n\n#### User accounts\nYour device should be supplied relatively factory-fresh, without existing user accounts.\n\nYou need to create a local device account for yourself, not linked to a Microsoft account, to use as a local\nadministrator account.\nYou should use your corporate network username for this account so that it can be unambiguously identified.\n\nSecure this account with a password.\n\n#### Ensure all the built-in threat protections are enabled\nModern Windows versions have highly capable firewall and anti-malware software, you must ensure that these remain\nenabled on your device.\n\nVery occasionally, it can be necessary to temporarily disable some of the threat protections, for example real-time\nfile scanning can sometimes prevent certain legitimate software from installing properly.\nHowever, whenever you do this you must re-enable them once the operation has completed.\n\n#### Disc encryption\nYou must activate BitLocker disc encryption on your device, with a minimum 8-digit PIN.\n\nYou must supply the recovery key and disc identifier to your line manager so they can keep a record of them.\n\n#### Public networks\nWhen you connect to public networks, ensure that you identify them as such to Windows so that it does not share\nresources from your device.\n\n#### Virtualisation\nMost development work should be undertaken in a dedicated virtual machine as this helps to protect the integrity of\nthe host device.\n\nCommercial virtualisation software can be provided by your line manager if required.\n\n#### Corporate resources access\nWhilst most corporate resources sit inside entirely private networks, it is possible for some of our online\nservices to be accessed from an unmanaged device.\nTo do this, additional controls can be applied to the device.\n\nTo allow your device to access the corporate Office 365 services, you will need to enrol it with InTune.\nThis facility can be requested through your line manager.\n\nIf you enrol your device, you will need to use your corporate account, which will become registered as a new account\non the device in addition to the local account you have already created.\nYou will need to secure this account with a PIN.\n\n#### Cloud development environment access (VPN)\nDedicated development environments and services can be accessed via VPN.\nYou should install the VPN client on your device according to instructions provided by the operations team.\n\nVPN client connections should be made from the host device, not from virtual machines.\n\n### Controls for Macbooks\n\n#### Don't set a firmware password\nFirmware passwords offer minimal additional protection and are easily lost or forgotten, which can only be fixed by\nan authorised Apple representative.\n\n#### Use the latest general release operating system version\nIt can sometimes take a while for application vendors to support new macOS versions and there's no going back, so you\ncan allow yourself a month or two before moving to the latest macOS operating system.\nBut you should upgrade as soon as it is safe to do so.\n\nIf you have a device that will not support the latest version of the operating system then you should talk to your\nline manager to arrange an alternative.\n\nYou must not install any other operating system such as Windows or a Linux distribution.\n\nIf you need to work with a different operating system then you must use a virtualisation technology.\n\n#### Keep the operating system and applications patched and up to date\nYou must ensure that your device is configured to automatically check for updates and you should regularly install\nmacOS and app updates so that you are using current features.\n\nMost importantly, you must ensure that your device is configured to automatically install system data files and\nsecurity updates, which only covers the built-in anti-malware software and its configuration.\n\n#### User accounts\nYour device should be supplied relatively factory-fresh, without existing user accounts.\n\nYou need to create a local device account for yourself to use as a local administrator account.\nYou should use your corporate network username for this account so that it can be unambiguously identified.\n\nSecure this account with a password.\n\nYou will also need to create an Apple ID registered to your corporate email address if you don't already have one.\nThis account can be used for the App Store and can be linked to your work iPhone if you have one, which is useful\nfor enabling two-factor authentication.\n\nHowever, you should not use this account to synchronise content across devices.\n\n#### Install dedicated anti-malware software\nAlthough Gatekeeper provides good protection for applications, it does not provide equivalent controls for other\nfiles.\n\nYou must install a commercial anti-malware product, which can be obtained from your line manager.\n\nVery occasionally, it can be necessary to temporarily disable some of the threat protections, for example real-time\nfile scanning can sometimes prevent certain legitimate software from installing properly.\nHowever, whenever you do this you must re-enable them once the operation has completed.\n\n#### Disc encryption\nYou must enable FileVault on the disc of your device and create a local recovery key.\nDo **not** use your iCloud account or store the recovery key with Apple.\n\nYou must supply the recovery key and disc identifier to your line manager so they can keep a record of them.\n\n#### Virtualisation\nMost development work will be done in the native operating system, but you may also wish to create virtual\nmachines.\n\n#### Corporate resources access\nWhilst most corporate resources sit inside entirely private networks, it is possible for some of our online\nservices to be accessed from an unmanaged device.\nTo do this, additional controls can be applied to the device.\n\nTo allow your device to access the corporate Office 365 services, you will need to enrol it with InTune.\nThis facility can be requested through your line manager.\n\n#### Cloud development environment access (VPN)\nDedicated development environments and services can be accessed via VPN.\nYou should install the VPN client on your device according to instructions provided by the operations team.\n",
      "repository_path": "docs/guides/unmanaged_devices.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:26.071000",
      "updated_at": "2025-01-24T11:12:26.071000"
    },
    {
      "_id": "6793759bdd8933f95f8d8f05",
      "text": "# Docker guidance\n\nA container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably across multiple environments.  Docker is a tool to build and run these containers.\n\n## More information\n[Docker introduction on docker.com](https://www.docker.com/resources/what-container)\n\n## Terminology\n`Dockerfile` - set of instructions for building a docker image  \n`Image` - a constructed set of layered docker instructions  \n`Container` - a running instance of an image\n\n## Multi stage builds\nDockerfiles should implement multi stage builds to allow different build stages to be targeted for specific purposes.  For example, a final production image does not need all the unit test files and a unit test running image would use a different running command than the application.\n\nBelow is an example multi stage build which is intended to use the Defra Node.js base image.\n\n```\nARG PARENT_VERSION=1.0.0-node12.16.0\nARG PORT=3000\nARG PORT_DEBUG=9229\n\n# Development\nFROM defradigital/node-development:${PARENT_VERSION} AS development\nARG PARENT_VERSION\nARG REGISTRY\nLABEL uk.gov.defra.parent-image=defradigital/node-development:${PARENT_VERSION}\nARG PORT\nENV PORT ${PORT}\nARG PORT_DEBUG\nEXPOSE ${PORT} ${PORT_DEBUG}\nCOPY --chown=node:node package*.json ./\nRUN npm install\nCOPY --chown=node:node app/ ./app/\nRUN npm run build\nCMD [ \"npm\", \"run\", \"start:watch\" ]\n\n# Production\nFROM defradigital/node:${PARENT_VERSION} AS production\nARG PARENT_VERSION\nARG REGISTRY\nLABEL uk.gov.defra.parent-image=defradigital/node:${PARENT_VERSION}\nARG PORT\nENV PORT ${PORT}\nEXPOSE ${PORT}\nCOPY --from=development /home/node/app/ ./app/\nCOPY --from=development /home/node/package*.json ./\nRUN npm ci\nCMD [ \"node\", \"app\" ]\n```\n\n## Docker Compose guidance\n\n### Use override files to reduce duplication\nAdditional settings can be applied to a docker compose file by using override files.\n\nOverride files can be applied by listing the files after the `docker-compose` command with the `-f` parameter, i.e.\n\n`docker-compose -f docker-compose.yaml -f docker-compose.override.yaml up`\n\nNote that the above is equivalent to running the command:\n\n`docker-compose up`\n\nas calling `docker-compose` without specifying any files will run `docker-compose` with any available `docker-compose.yaml` and `docker-compose.override.yaml` files in the executing directory. \n\nNote however that:\n\n`docker-compose up -f docker-compose.yaml`\n\nwill **not** apply the docker `docker-compose.override.yaml` file, only the file specified.\n\nOne use case is for running tests in CI - common settings can be put into the base `docker-compose.yaml` file, while changes to the command and containers needed in local development can be placed in override files.\n\nThe below example demonstrates changing the command and container name for testing:\n\n`docker-compose.yaml`\n\n```\nversion: '3.4'\nservices:\n  ffc-demo-service:\n    build: .\n    image: ffc-demo-service\n    container_name: ffc-demo-service\n    environment:\n      DEMO_API: http://demo-api\n\nvolumes:\n  node_modules: {}\n\n```\n\n`docker-compose.test.yaml`\n```\nversion: '3.4'\nservices:\n  ffc-demo-service:\n    command: npm run test\n    container_name: ffc-demo-service-test\n```\n\nThe tests can be run by providing the `docker-compose.test.yaml` file with a `-f` parameter:\n\n`docker-compose up -f docker-compose.yaml -f docker-compose.test.yaml`  \n\nIt is also recommended not to expose any ports through Docker Compose used in CI as they may conflict with other ports already in use in the build agent.  \n\nFurther documentation on docker-compose can be found at https://docs.docker.com/compose/reference/overview/#specifying-multiple-compose-files.\n\n### Use projects to provide unique volumes and networks\nTo avoid conflicts when running different permutations of docker files, projects should be specified to segregate the volumes and networks.\n\nThis can be achieved with the `-p` switch when calling docker compose on the command line.\n\n i.e. to start the service\n\n`docker-compose -p ffc-demo-service -f docker-compose.yaml up`\n\nand to run the tests\n\n`docker-compose -p ffc-demo-service-test -f docker-compose.yaml -f docker-compose.test.yaml up`\n\n### Use environment variables to guarantee unique projects and containers\nWhen running through CI, a combination of the `-p` switch and environment variables can be used to ensure each build and test has unique project and container names.  This will prevent conflicts with other build pipelines when using tools such as a single node Jenkins.\n\nFor example using Jenkins, the following compose files can be started via:\n\n`docker-compose -p ffc-demo-service-$PR_NUMBER-$BUILD_NUMBER -f docker-compose.yaml up`\n\nand tested with\n\n`docker-compose -p ffc-demo-service-test-$PR_NUMBER-$BUILD_NUMBER -f docker-compose.yaml -f docker-compose.test.yaml up`\n\nusing `PR_NUMBER` and `BUILD_NUMBER` environment variables to isolate build tasks.\n\n`docker-compose.yaml`\n\n```\nversion: '3.4'\nservices:\n  ffc-demo-service:\n    build: .\n    image: ffc-demo-service\n    container_name: ffc-demo-service-${PR_NUMBER}-${BUILD_NUMBER}\nvolumes:\n  node_modules: {}\n\n```\n\n`docker-compose.test.yaml`\n```\nversion: '3.4'\nservices:\n  ffc-demo-service:\n    command: npm run test\n    container_name: ffc-demo-service-test-${PR_NUMBER}-${BUILD_NUMBER}\n```\n\n### Composing multiple repositories for local development\nFor scenarios where multiple containers need to be created across multiple repositories, it might be advantageous to create a \"development\" repo.\n\nThe development repository would:\n- clone all necessary repositories\n- builds images from Dockerfiles in each repository by referencing Docker Compose files in those repositories\n- run containers based on those images in a single Docker network by referencing Docker Compose files in those repositories\n- run single containers for any shared dependencies across repositories such as message queues or databases\n\nTo facilitate this, each repository with a potentially shared dependency will need its Docker Compose override files to be setup in such a way that dependency containers can be isolated.  This will allow those repository services to run both in isolation and as part of wider service depending on development needs.\n\nFor example, let's say we have two repositories, **ServiceA** and **ServiceB**.  **ServiceA** communicates with **ServiceB** via an ActiveMQ message queue. **ServiceB** has a PostgreSQL database.\n\n**ServiceA**'s Docker Compose files could be structured as follows.\n\n`docker-compose.yaml` - builds image and runs **ServiceA**\n`docker-compose.override.yaml` - runs Artemis ActiveMQ container\n`docker-compose.link.yaml` - runs **ServiceA** in a named Docker network\n\n**ServiceB**'s Docker Compose files could be structured as follows.\n\n`docker-compose.yaml` - builds image and runs **ServiceB** and PostgreSQL container\n`docker-compose.override.yaml` - runs Artemis ActiveMQ container\n`docker-compose.link.yaml` - runs **ServiceB** in a named Docker network\n\n**ServiceA** and **ServiceB** can be run in isolation by running the following commands in each repository.\n\n`docker-compose build`\n`docker-compose up`\n\nThe development repository would contain the following.\n\n`docker-compose.yaml` - runs Artemis ActiveMQ container in named Docker network\n\nA script which would run the following commands:\n\n```\nif [ -z \"$(docker network ls --filter name=^NETWORK_NAME$ --format={{.Name}})\" ]; then\n  docker network create NETWORK_NAME\nfi\ndocker-compose up\ndocker-compose -f path/to/ServiceA/docker-compose.yaml -f path/to/ServiceA/docker-compose.link.yaml up --detach\ndocker-compose -f path/to/ServiceB/docker-compose.yaml -f path/to/ServiceB/docker-compose.link.yaml up --detach\n```\n\n#### Avoiding docker-compose.yaml in the development repository\nIf it is preferred to avoid the need for an additional `docker-compose.yaml` file in the development repository itself, an alternative approach would be to explicity declare the shared resources are not started in subsequent `override` files in the start up script.\n\nFor example:\n\n```\nif [ -z \"$(docker network ls --filter name=^NETWORK_NAME$ --format={{.Name}})\" ]; then\n  docker network create NETWORK_NAME\nfi\ndocker-compose up\ndocker-compose -f path/to/ServiceA/docker-compose.yaml -f path/to/ServiceA/docker-compose.override.yaml -f path/to/ServiceA/docker-compose.link.yaml up --detach\ndocker-compose -f path/to/ServiceB/docker-compose.yaml -f path/to/ServiceB/docker-compose.override.yaml -f path/to/ServiceB/docker-compose.link.yaml up --detach --scale SERVICE_NAME=0\n```\n\n### Binding volumes to container\nTo aide local development, the local workspace can be bound to a Docker volume.  This allows code changes to be automatically picked up within the container without the need to rebuild the image or restart the container.  \n\nTo best support this, workspaces should be structured so it is simple to determine which files should be bound to Docker volumes as it would not be appropriate to bind everything.  For example, it would not be beneficial to bind `node_modules` or a `README`.  \n\nExample of Docker compose file with volume binding.  \n\n```\nvolumes:\n  - ./app/:/home/node/app/\n  - ./test/:/home/node/test/\n  - ./test-output/:/home/node/test-output/\n  - ./package.json:/home/node/package.json\n```\n\nChanges to any of the directories listed above would automatically be picked up in the running container.\n\nBinding also allows developers to take advantage of file watching in testing applications.  Changes made to code locally will automatically be reflected in the running container supporting a TDD approach.\n\n### .dockerignore\nA `.dockerignore` file is a way of preventing local files being copied into an image during build.\n\nFor example, if a repository contains the following files.\n\n```\napp/index.js\napp/config.js\nnode_modules\nindex.js\nREADME.md\nLICENCE\nDockerfile\n```\n\nThe `Dockerfile` in this repository includes the following layer which would copy all local files to the container.\n\n```\nCOPY . .\n```\n\nWhen the image is built then all files in the repository are copied to the image.  In this scenario, it is not ideal for performance and disk space reasons to copy the `node_modules`, `LICENCE`, `Dockerfile` or `README.md` to the image.\n\nTo prevent this a `.dockerignore` file should be added with the following content.\n\n```\nnode_modules\nDockerfile\nLICENCE\nREADME.md\n```\n\n### Container and image names using Docker Compose\nIf an image name or container name is not specified in a Docker Compose file, then Docker Compose will determine it's own based on the service name.  This can result in duplication in the name and unpredictabilty in futher container interaction.\n\n#### Set image and container name\n\n```\nversion: '3.7'\nservices:\n  my-service:\n    image: my-service\n    container_name: my-service\n```\n\n### Preserving database volumes during test runs\nIn many scenarios it is beneficial to utilise Docker to run local integration tests against a containerised dependency such as a database or message broker.\n\nThese tests would typically write and delete data during test execution.  In order to prevent this impacting on local development data and still avoid duplication in Docker Compose definitions, volumes should be declared separate to the database definition.\n\nFor example, if you have the following Docker Compose files\n\n- `docker-compose.yaml` - base definition used in all scenarios\n- `docker-compose.override.yaml` - applied when running locally only\n- `docker-compose.test.yaml` - applied when running tests only\n\nThen using a Postgres image as an example each definition should contain the following.\n\n#### docker-compose.yaml\n```\nversion: '3.7'\nservices:\n  my-postgres-service:\n    image: postgres:11.4-alpine\n    environment:\n      POSTGRES_DB: my_database\n      POSTGRES_PASSWORD: postgres\n      POSTGRES_USERNAME: postgres      \n```\n\n#### docker-compose.override.yaml\n```\nversion: '3.7'\nservices:\n  ffc-demo-claim-postgres:\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data: {}\n```\n\nThen volume and port bindings are only used during local development and any local tests runs will not impact development data.\n",
      "repository_path": "docs/guides/docker_guidance.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:27.478000",
      "updated_at": "2025-01-24T11:12:27.478000"
    },
    {
      "_id": "6793759cdd8933f95f8d8f06",
      "text": "# Style guide for software development standards\n\nThis guide tells you how to write and format standards and other documents for this repository.\n\nWe want our standards to be:\n\n- clear\n- consistent\n- easy to read\n\nIf a standard is easy to understand, it's also easier to follow it correctly.\n\n[Guidelines for writing for the web](https://www.bath.ac.uk/guides/writing-for-the-web/)\n\n## Be clear and focused\n\nA standard should set clear rules. A guide should give clear advice.\n\nBe concise. Focus on what is most important. Avoid woolly language and excessive detail.\n\nIt should be straightforward to answer \"Does this project meet the standard?\" with a \"yes\" or \"no\". Basically, in developer terms, a standard should be easily testable.\n\n## Be easy to skim\n\nWe'd love everyone to read every last word of these standards. But in reality, people often skim the content they read on the web.\n\nTo make content easier to skim:\n\n- use short sentences and paragraphs\n- break up writing into multiple sections with clear subheadings\n- display information in bulleted lists\n- put the most important information first\n\nThis helps users to quickly understand the main points of a standard. It also makes it easier to find the specific information they're looking for.\n\n## Use plain English\n\nOur guides and standards should use plain English whenever possible. What we write about can be complex, but the way we write about it doesn't have to be.\n\nWe are writing for a specialist audience. But [even specialists want content to be easy to read](https://gds.blog.gov.uk/2014/02/17/guest-post-clarity-is-king-the-evidence-that-reveals-the-desperate-need-to-re-think-the-way-we-write/):\n\n> when given a choice, 80% of people preferred sentences written in clear English ... the more specialist their knowledge, the greater their preference for plain English\n\nIt's fine to use technical terminology, but avoid corporate jargon.\n\n## Check your writing for clarity\n\n[Hemingway](http://www.hemingwayapp.com/) is an online tool that helps make writing easier to read.\n\nIt makes suggestions for when to simplify vocabulary or split up sentences that are too long.\n\nYou don't have to obey every suggestion from Hemingway, but it can be a useful guide.\n\n## Use Markdown for formatting\n\nThere are a few variations of Markdown, so we use the standard GitHub syntax.\n\n[Read GitHub's formatting guidance](https://help.github.com/en/github/writing-on-github/basic-writing-and-formatting-syntax)\n\nName each file using snake case (for example, `style_guide_for_standards.md`).\n",
      "repository_path": "docs/guides/style_guide_for_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:28.432000",
      "updated_at": "2025-01-24T11:12:28.432000"
    },
    {
      "_id": "6793759ddd8933f95f8d8f07",
      "text": "# SQL Prompt\n\nAt the moment we are trialing the use of [SQL Prompt](https://www.red-gate.com/products/sql-development/sql-prompt/) to perform code analysis and formatting. This was pretty much the only tool we could find that could format and analyse T-SQL that worked. We tried a few free ones but these were not very good and only did basic formatting, not code analysis. A 28 day trial version can be downloaded from <https://www.red-gate.com/products/sql-development/sql-prompt/>.\n\nSQL Prompt has the following funtionality\n\n- Advanced IntelliSense-style code completion\n- Code snippet library\n- Refactor SQL code\n- Fast and comprehensive code analysis\n- SSMS tab history and coloring\n- Advanced customizable and sharable code formatting\n\nFollow these links for SQL Prompt's [Documentation](https://documentation.red-gate.com/sp9) and the [Code Analysis Rules](https://documentation.red-gate.com/codeanalysis) it uses.\n\nWe trialed SQL Prompt over 2 weeks and have found it very good at tidying up code and making it consistent. We also found thats it's other features made writing T-SQL quicker & easier. Although we were happy with its settings straight out of the box these can be changed and shared easily.\n\nMore trialing by other T-SQL developers would be useful before any decisions are made.\n",
      "repository_path": "docs/guides/sql_prompt_tool.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:29.448000",
      "updated_at": "2025-01-24T11:12:29.448000"
    },
    {
      "_id": "6793759edd8933f95f8d8f08",
      "text": "# Choosing packages and dependencies\n\nWe use a lot of packages and dependencies in our services. Using an established package can be a great way to fix a problem or add a feature. It can be quicker and easier than writing the code from scratch. And it can make your application more consistent with other Defra services and industry standards.\n\nWhen you are picking a package or a dependency to include in your project, there are some things to consider:\n\n## What have other Defra services used?\n\nIs there an agreed-upon standard package for this purpose? If there is, use it.\n\nFor example, Hapi is our Node.js framework of choice. So if you're starting a new Node project and need a web framework, use Hapi over alternatives like Express.\n\nIf there's no official standard, you should still consider what similar services use. Codebases can be more consistent and easier to maintain if they use the same packages.\n\nIt can also be quicker to introduce an already-used package to your project. You can reuse code from our other services, or learn from developers who have worked with it.\n\n## What are the most popular solutions?\n\nIf you're looking for a package for a common function (like logging or user authentication), there may be many potential candidates.\n\nWhen in doubt, go for the most used package, especially if it's become the industry standard.\n\nYou can compare how popular packages are by looking at the number of downloads they have on package manager sites (like [NPM](https://www.npmjs.com/) or [RubyGems](https://rubygems.org/)). You can also check how many times the repository has been watched or starred on GitHub.\n\nA well-established package will usually have a lot more guidance and support. There will be more blog posts, more questions on Stack Overflow, and other projects you can refer to.\n\nIt also makes it easier to introduce new developers to your service, as it's more likely they'll be familiar with an industry standard.\n\nThis isn't a hard rule. If the most popular package doesn't meet your needs, or has clear issues with security or maintainability, then it's OK to go for an alternative. But if you're choosing between one package with a thousand downloads and another with a million downloads, you should go for the second one.\n\n## Is the package maintained?\n\nCheck the package history. How often is it updated? Who is updating it?\n\nPoorly maintained packages are more likely to have security problems. They may not be upgraded to work with newer versions of your language, framework and other dependencies.\n\nA package with a single maintainer is also a much riskier choice than one managed by a team.\n\nIf the package has a GitHub issues section, or a forum, check if the maintainers respond to problems. If reported issues never get fixed, then it's time to consider alternatives.\n\n## Is the package secure?\n\nCheck the package history. Is there evidence that the maintainers have applied security fixes? Do they regularly update their dependencies?\n\nMany GitHub repositories will have READMEs with badges for their security CI tools. Check to see if there are any warnings or known security issues.\n\n## Is the package documented?\n\nIs there a thorough README or a Wiki? It should be clear how you can install and configure the package, and how to use its features and API.\n\n## Is the package compatible with our licence?\n\nOur open source applications should use the [Open Government Licence](http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/). So any packages they include should be compatible with this licence.\n\nThe National Archives has more [guidance about open software licences](http://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/uk-government-licensing-framework/open-government-licence/open-software-licences/).\n\nAlso it's important to know that if the package does not have a licence ***you cannot use it***.\n",
      "repository_path": "docs/guides/choosing_packages.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [
        "6790cb8170024c99a94725a5"
      ],
      "created_at": "2025-01-24T11:12:30.544000",
      "updated_at": "2025-01-24T11:12:30.544000"
    },
    {
      "_id": "679375a0dd8933f95f8d8f09",
      "text": "# Mobile application guidance\n\n## Introduction\n\nThe huge increase in the use of mobile devices means that mobile application development is now of critical interest to the Defra family, even more so given the amount of field-work being carried out by workers within the Defra family.\n\n## Scope\n\nThis guidance applies to mobile application in-house development and management.\n\nThis guidance is intended to cover all forms of mobile application: from fully \"off-line capable\" progressive web applications, through cross-platform apps to fully native apps.\n\nThis document should be read in conjunction with the Mobile Application Standards (**[Mobile Application Standards](../standards/mobile_app_standards.md)**).\n\n## Caveats\n\nThis is a fast moving area involving multiple platforms and diverse technology stacks: this guidance is early and provisional.\n\n## Extra security guidance\n\nPlease ensure that you have read the \"Extra Security Considerations\" of the **[Mobile Application Standards](../standards/mobile_app_standards.md)** before reading this section.\n\n### Application wrappers\n\nThe Mobile Application Standards refer to various mechanisms for encrypting traffic going to and from the app over usually public networks: specifically HTTPS/Transport Layer Security, use of Virtual Private Networks or use of an in-built application security wrapper. The potential use of in-built application security wrappers requires some further guidance. This technique, often called \"application wrapping\", is a service commonly offered by Mobile Device Management (MDM) software suites. It involves using the MDM suite in some way to allow the MDM software to add an extra layer of security to your application bundle, such as encryption for data traffic (based on standard transport layer security/secure sockets, i.e. shared and verifiable certificates) or locally stored application data (with that encryption usually based on a secret known only to the user, such as their device passcode). Another important advantage of application wrapping is to lock-down access, either to data stored on the device or remote systems, on a per app basis.\n\nWhile the use of this technique cannot be entirely ruled out, it is declining in popularity as it has many disadvantages. Application wrapping complicates deployment, creating an extra step and meaning that the wrapped app may not work properly outside the MDM eco-system. Even worse, it involves allowing the \"application wrapping service\" to make last minute changes to your application bundle, in ways that are largely outside of your control and, because they involve security, are likely to be deliberately obscure. It is even possible that the \"application wrapping service\" will itself be changing in ways unknown to us, often in response to continuously evolving security threats, so that an app wrapped at one moment may not be identical to one wrapped at the next, even if we have not ourselves made any changes to the app. Should the app wrapping itself cause any issues with the working of the app, those issues can therefore be extremely difficult to resolve.\n\nApp wrapping should therefore only be adopted if it is deemed really necessary and otherwise unavoidable. On Apple devices, which natively run each app in its own \"sandbox\", so that apps cannot access each other's data, access to data stored locally on the device is already locked down on a per app basis. That means that, on Apple devices, the remaining security hole potentially pluggable by app wrapping is the handling of data traffic and access to remote systems. However, Mobile Device Management suites offer a simpler way to plug that hole: a technique called per app or in-app VPN, where the app is forced to use Virtual Private Network software provided by the MDM suite to both encrypt traffic and allow the app access to remote systems that would otherwise be blocked. In-app VPN is therefore the default approach for Apple devices, and it is very unlikely per app wrapping should ever be necessary in that case. (NB we are talking here about a per-app VPN app shipped with our adopted Mobile Device Management suite, and configured for internal use. These are not to be confused with third party VPN apps that are generally available, and which should normally be avoided as they often harvest your data).\n\nConversely the degree of protection offered by Android app sandboxing is version dependent and regarded as less secure, so, for an app using Android devices, there is a possible case for wrapping, if, in this instance, the wrapper technology actually adds value and the data in scope requires it.\n\n### Protecting data at rest on the device\n\nMobile applications and associated data held locally on devices will be subject to exactly the same heightened physical security risks as the devices on which the apps are running. Mobile devices are relatively easily lost or stolen. This means **it is very important to consult the latest security standards as to how data stored on the device should be protected, and, if the application is intended for internal use on a managed device, that you consult with the Cloud Mobile Services team in Group Infrastructure and Operations to find out what the Mobile Device Management (MDM) software used to manage the device may mean for how you should implement your app.** For example, you may discover \"out of the box\" Mobile Device Management features that can be utilised to better protect your app's data, such as the ability to force users of managed devices to set more secure passcodes before they can log into the device and decrypt its data.\n\n### Authenticating external users\n\nMobile apps are increasingly being used by organisations as a way to provide publicly accessible services. Sometimes, such publicly accessible apps also need to support a form of authentication. This is likely to be more challenging than adopting authentication for systems built for internal use, where internal authentication systems and associated business processes are likely to be well established. Furthermore, any authentication mechanisms adopted by the mobile app must be capable of operating securely within one of the most insecure contexts conceivable: on an unmanaged mobile device used by any member of the general public. **Developers and application architects must therefore research, consult and very carefully decide as to the authentication mechanisms the application will need to adopt.**\n\nTo that end, bear in mind the following points. Authentication that is established by linking the user to, say, a real email address, by sending the user an email to that address and making the user respond, is relatively easy to implement, but be aware that, if the user is required to prove their actual identity, that can be much more problematic and involved. Please consult the latest security standards, and bear in mind that even already available verification services such as gov.uk.Verify are not without their own very significant challenges. Implementing reliable ways of allowing someone to prove their real identity is simply a very difficult task. Given this fact it would be worth considering if, in reality, an app currently considered to be for use by the \"general public\" is really intended to be used by parties already known to the Defra family, such as contracted companies or suppliers, or regulated entities who already hold licences issued by the Defra family. In such a scenario, where the target users are already known, a \"by invitation\" pattern of establishing authentication by sending details to an already officially recorded email or physical address may provide sufficient assurance that we know the real identity of the user with whom we have engaged.\n\n## Extra guidance on Progressive Web Apps\n\nUnfortunately, the Defra family mandating of Apple devices does have some negative implications for Progressive Web Apps, of which we need to be aware. Apple force the use of the Safari/WebKit browser on all iOS/iPad OS devices, and, of all the major browsers, Safari has the most limited support for Progressive Web Apps. There is no way around this as even other browser apps in the Apple app store, such as Chrome, are merely wrappers for WebKit. The most notable restrictions are likely to be around size of storage (though, in January 2020, and if using IndexedDB, which is usually the most functional local storage option in any case, over 1 GB was found to be available on a Defra iPad), persistence of stored data (as of July 2020 you can depend on IndexedDB data being retained for at least a month, and possibly much longer, even if an app is completely unused, and data has been observed to survive an iPad OS and Safari update. This therefore seems to be a diminishing issue, though one of which you should be aware.) and the fact that, in Safari on iOS or iPad OS, the way you install an app on the home screen is still currently non-standard.\n\nIt is possible to build a perfectly useful PWA that runs on Safari, including being able to access native features such as taking photos (though, at the time of writing, only stills and not videos) and accessing geolocation. However, the situation is always changing, usually improving, though that cannot be guaranteed, so you must use a site such as <https://whatwebcando.today/> to assess which features are actually available in Safari on a Defra Apple device before committing to delivering a PWA. Similarly, you should assess the current storage capabilities of Safari on a device by using a web site such as <https://demo.agektmr.com/storage/>.\n\nOf course, PWAs are storing data locally: otherwise they cannot work offline. That means that you will also need to consider the security of the data at rest on the device. These considerations are slightly different for PWAs than for other apps, as the data is being retained in the browser. If, as is likely and recommended, you are using IndexedDB to store your data, then all browsers compliant with the W3C spec, such as Safari, will only allow JavaScript from the same origin as the PWA that has created the database to access that data. Also, your data will be protected by your device's normal data encryption: for example, for an internally managed Defra iPad where the setting of a passcode is enforced, that means the data will be encrypted whenever the device is turned off, and only decrypted when the user re-enters the correct passcode. That means \"reasonably\" secure, but most likely not as secure as Apple's app sandboxing, and this should be borne in mind when assessing whether or not a PWA is appropriate, given the sensitivity, or not, of the data it will be handling, and the general security features of the devices where the app will be in use.\n\n## Building cross-platform apps\n\nIt is possible that, having run through the hierarchy of Mobile Application development options as specified in the Mobile Application Standards the decision will have been reached to build a mobile app using a cross platform technology stack. These stacks are ways of using the same, or very close to the same, code-base but still build the app in ways that can allow it to run on multiple platforms and multiple devices. Otherwise a completely different code base, using a different language and development framework, is needed to build an app for each platform.\n\nThere are many such cross platform technology stacks, but some are obscure or are nearing obsolescence. **The only cross platform stacks that should be considered for new projects, from the newest to the oldest, are Flutter, React Native or Xamarin.** All are backed by major vendors: Flutter by Google, React Native by Facebook and Xamarin by Microsoft, so all should have a reasonable shelf life, with Xamarin possibly the most \"at risk\" as it the oldest and has been overtaken by React Native and Flutter in terms of popularity (according to a 2018 Stack Overflow survey). Flutter requires developers to learn its own programming language, \"Dart\", though this is based on Java and C\\# (as this is an open eco-system it is impossible to mandate an IDE, but Android Studio or Visual Studio Code are popular choices). React Native uses JavaScript as its programming language but also the whole React framework including a presentational technology called JSX, so is a relatively easy transition for React developers, but not so much for developers who know JavaScript but are not familiar with React (here again an IDE cannot be mandated, but Atom and Visual Studio Code are popular choices). Xamarin uses C\\# so ought to be a reasonable match for .NET developers. Xamarin will require you to use full-blown Visual Studio for the IDE.\n\nA cross platform approach built using a modern framework such as Flutter, React Native or Xamarin will very usually allow for the building of a suitably reliable and functional mobile app. As a rough rule of thumb, the more mature the framework the more capable it is likely to be in this regard, as, not only will the framework have had more time to include the desired features but, if those features are not already available \"out of the box\", it is also more likely that a third party library will be available for use that does the job. However, cross platform apps may not provide the exact look and feel and performance of a native app (out of Flutter, React Native and Xamarin, \"React Native\" is likely to give the most native look and feel, as it's uses native presentational components, despite integrating with each platform in, out of the three platforms, the least \"native\" way. React Native integrates via each platform's native JavaScript engine, where-as Flutter and Xamarin both run within an engine of their own, the \"Shell plus Dart Virtual Machine\" for Flutter and \"Mono\" for Xamarin, that interact directly with native libraries). Even more importantly, it is just about possible that the framework may not be able to access some native functionality of the device, so you are advised to establish that any crucial native functionality will be available to the app before committing to a certain development option.\n",
      "repository_path": "docs/guides/mobile_app_guidance.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [
        "6790cb8170024c99a94725a5",
        "678fcc01c24985192f15edc6",
        "678fcc2bc24985192f15edcc"
      ],
      "created_at": "2025-01-24T11:12:32.194000",
      "updated_at": "2025-01-24T11:12:32.194000"
    },
    {
      "_id": "679375a1dd8933f95f8d8f0a",
      "text": "# Security principles\n\nSecure development principles. These are based on ones provided by [NCSC](https://www.ncsc.gov.uk)\n\nFor more details visit <https://www.ncsc.gov.uk/collection/developers-collection?curPage=/collection/developers-collection/principles>\n\n## Secure development is everyone's concern\n\nGenuine security benefits can only be released when delivery teams weave security into their everyday working practices.\n\n## Keep your security knowledge sharp\n\nCreating code that is capable of withstanding attack requires an understanding of attack types and of defensive security practices.\n\n## Produce clean & maintainable code\n\nIf your code lacks consistency, is poorly laid out and undocumented, you're adding to the overall complexity of your system.\n\n## Secure your development environment\n\nThere is sometimes a perceived conflict between security and usability. This situation is highlighted in the case of end user devices and the environments used to support software development.\n\n## Protect your code repository\n\nYour code is only as secure as the systems used to create it. As the central point at which your code is stored and managed, it's crucial that the repository is sufficiently secure.\n\n## Secure the build and deployment pipeline\n\nContinuous integration, delivery and deployment are modern approaches to the building, testing and deployment of IT systems.\n\n## Continually test your security\n\nSecurity testing can be manual, but it can also be automated.\n\n## Plan for security flaws\n\nAll but the very simplest software is likely to contain bugs, some of which may have a security impact.\n",
      "repository_path": "docs/principles/security_principles.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:33.418000",
      "updated_at": "2025-01-24T11:12:33.418000"
    },
    {
      "_id": "679375a3dd8933f95f8d8f0b",
      "text": "# Coding principles\n\nThese are common principles that apply to coding.\n\n## Rationale\n\n- Ensure consistency across coding standards for different languages\n- Provide a reference point to justify coding standards\n\n## Principles\n\n### Master is always shippable\n\n> The main branch(s) of a project is always in a state of 'shippable to production'\n\nWe always maintain a golden copy of our code that is considered complete and production ready.\nIt has sufficient tests, matches agreed styles, doesn't break the build, has documentation, and is our best solution at the time of committing.\n\n- This allows us to iterate safely ('Start small')\n- It minimises problems during delivery\n- It prevents overload of work due to mixing of production issues and feature development ('Minimise work in progress')\n\n### The code is not yours\n\n> We write and commit code thinking of someone else, not ourselves\n\nWhen writing code you should be thinking of the next developer who has to work with it, who may be your future self, another member of the team, or someone completely new.\n\nThey will not have the understanding and context you do at the time of writing it, so when they're amending or refactoring your work later it is important they can derive the intent of what your code is doing.\n\n### Work in the open\n\n> Our code is open as early as possible. We only go private if we really, really, **really** have to!\n\n- Encourages good discipline\n- Increased sharing and collaboration\n- It makes things better\n- It improves the overall security of code\n\n### Code securely\n\n> We incorporate good security practice in all our code\n",
      "repository_path": "docs/principles/coding_principles.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:35.003000",
      "updated_at": "2025-01-24T11:12:35.003000"
    },
    {
      "_id": "679375a4dd8933f95f8d8f0c",
      "text": "# Security Standard\n\nHaving a secure approach to development has never been so important.\n\nThe way we build software and systems is rapidly evolving, becoming more and more automated and integrated. This results in a need to have some standards and guidance around security. Rather than maintain our own, we follow the standards of [OWASP](https://www.owasp.org/index.php/Main_Page)\n\n## Standards\n\nUse the [OWASP Secure coding practices - quick reference guide](https://www.owasp.org/index.php/OWASP_Secure_Coding_Practices_-_Quick_Reference_Guide) for details of the standards to apply.\n\n**Important note.** We are using version 2\n",
      "repository_path": "docs/standards/security_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:36.142000",
      "updated_at": "2025-01-24T11:12:36.142000"
    },
    {
      "_id": "679375a5dd8933f95f8d8f0d",
      "text": "# C# coding standards\n\nThese are common standards that apply to coding when using C# and are designed to be used in conjunction with the overall common coding standards.\n\n## Rationale\n\n - Ensure consistent C# code and styling across all projects\n\n### Resource\n\n[Microsoft's standard conventions](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/inside-a-program/coding-conventions)\n\n### Naming Conventions\n\n- PascalCasing for class names and method names\n\n```csharp\npublic class ClientActivity\n{\n    public void ClearStatistics()\n    {\n        // ...\n    }\n\n    public void CalculateStatistics()\n    {\n        // ...\n    }\n}\n```\n\n- camelCasing for method arguments and local variables\n\n```csharp\npublic class UserLog\n{\n    public void Add(LogEvent logEvent)\n    {\n        int itemCount = logEvent.Items.Count;\n        // ...\n    }\n}\n```\n\n- do not use underscores in identifiers (you can prefix private static variables with an underscore)\n- use predefined type names instead of system type names for example\n\n```csharp\n// Correct\nstring firstName;\nint lastIndex;\n\n// Avoid\nString firstName;\nInt32 lastIndex;\n```\n\n- use noun or noun phrases\n\n```csharp\npublic class Employee\n{\n\n}\n\npublic class BusinessLocation\n{\n\n}\n```\n\n- prefix interfaces with the letter `I`.\n\n```csharp\npublic interface IShape\n{\n\n}\n```\n\n### Layout Conventions\n\n[CodeMaid](http://www.codemaid.net/) is a free open source Visual Studio Extension which will cleanup and simplify your C# code. It can be downloaded as an extension to visual studio and can run automatically either on save or on demand on either a selection of code or a whole solution.\n\n1.  Remove unused using statements\n2.  Sort using statements\n3.  Add unspecified access modifiers\n4.  Remove empty regions\n5.  Add blank line padding\n6.  Remove blank lines next to braces\n7.  Run Visual Studio formatting\n8.  Remove consecutive blank lines\n9.  Remove end of line whitespace\n10.  Update endregion tags\n\nThe default settings for CodeMaid should remain in place to adhere to [DEFRA principles](../principles/README.md) (details at http://www.codemaid.net/documentation/)\n\n### Testing\n\nAs per the general coding standards it is expected that 90% of code is covered with unit tests however given unit test coverage in some C# projects (i.e. MVC) will include coverage of default Microsoft template code this 90% figure can be misleading. It is not expected that unit tests should be written to cover code already written by Microsoft and therefore attributes can be added to this code to exclude them from the code coverage calculation.\n\nFor example in a typical MVC application exclusions can be applied to areas such as anything in `App_Start` folder, any migrations, `global.asax`, etc.\n\nTo exclude test code from the code coverage results and only include application code, add the `ExcludeFromCodeCoverageAttribute` attribute to your test class.\n\nTo customize code coverage, follow these steps:\n\n1. Add a run settings file to your solution. In Solution Explorer, on the shortcut menu of your solution, choose Add > New Item, and select XML File. Save the file with a name such as `CodeCoverage.runsettings`.\n2. Add the content from the [example file](https://docs.microsoft.com/en-us/visualstudio/test/customizing-code-coverage-analysis?view=vs-2017#sample-runsettings-file), and then customize it to your needs as described in the sections that follow.\n3. To select the run settings file, on the Test menu, choose Test Settings > Select Test Settings File. To specify a run settings file for running tests from the command line or in a build workflow, see Configure unit tests by using a `.runsettings` file.\n\nWhen you select Analyze Code Coverage, the configuration information is read from the run settings file.\n\nTo turn the custom settings off and on, deselect or select the file in the Test > Test Settings menu.\n\nhttps://docs.microsoft.com/en-us/visualstudio/test/customizing-code-coverage-analysis?view=vs-2017\n",
      "repository_path": "docs/standards/csharp_coding_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [
        "678fcc2bc24985192f15edcc",
        "678fcc01c24985192f15edc6"
      ],
      "created_at": "2025-01-24T11:12:37.803000",
      "updated_at": "2025-01-24T11:12:37.803000"
    },
    {
      "_id": "679375a7dd8933f95f8d8f0e",
      "text": "# Ruby coding standards\n\nUse the latest version of Ruby 2.4 when starting a new project.\n\nWe follow the [Ruby style guide](https://github.com/bbatsov/ruby-style-guide).\n\n## Checking your code style\n\nInclude [defra-ruby-style](https://github.com/DEFRA/defra-ruby-style) in your Ruby projects. We built this gem to check and enforce code style.\n\nOnce you have the gem installed, you can check your code for issues by running `bundle exec rubocop`. If you have a linter in your text editor, it will also use the gem.\n\nInclude `bundle exec rubocop` in your CI, and the build will fail if there are any style violations.\n\n## Tools\n\nUse [Hakiri](https://hakiri.io/) to check for vulnerabilities in your project and dependencies. Integrate it through GitHub and it will check new commits and PRs.\n\n## Rails\n\nUse Rails 4.2 when starting a new project.\n\nWe follow [Rails best practice](https://github.com/bbatsov/rails-style-guide).\n",
      "repository_path": "docs/standards/ruby_coding_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:39.486000",
      "updated_at": "2025-01-24T11:12:39.486000"
    },
    {
      "_id": "679375a8dd8933f95f8d8f0f",
      "text": "# Kubernetes standards\n\n## Standards\n### Use a managed Kubernetes service\nManaged Kubernetes services such as Azure Kubernetes Service (AKS) in Azure or Elastic Kubernetes Service (EKS) in AWS are used as opposed to any IaaS Kubernetes implementation.\n\nThis is because managed Kubernetes services abstract the maintenence and configuration of master nodes to the cloud provider, meaning teams only need to support the worker nodes where services run.  Maintaining a full Kubernetes cluster can be very complicated and requires a high level of in depth Kubernetes and networking knowledge which can be a barrier to entry for some teams.  Using a managed service significantly reduces this complexity.\n\n### Use Helm for packaging deployments\n[Helm](https://helm.sh/) is a tool to bundle individual Kubernetes configuration files into single deployable packages.  This significantly reduces the complexity of configuring a cluster and deploying applications to it.\n\nHelm 3 must be used and teams should never use Helm 2 due to security risk introduced by running Tiller in a cluster.\n\nTeams are also encouraged to use a Helm Library chart to reduce duplication of Helm charts across multiple services.  Such as [this one](https://github.com/DEFRA/ffc-helm-library) used by the Future Farming and Countryside Programme (FFC).\n\n### Use ConfigMaps for configuration\nConfiguration for an application running in a pod should be passed to the pod via a `ConfigMap` Kubernetes resource. \nThe `ConfigMap` is more flexible than just using environment variables alone, and as well as supporting file based values, allows decoupling of pod definitions from configuration definitions.\n\nEnvironment specific values should be overriden during the Helm deployment.\n\nSensitive values should never be passed to a `ConfigMap`.\n\n### Secrets\nWhere possible, secrets should not be stored within an application or Kubernetes pod.  Instead, when communicating with supported cloud infrastructure, clusters should use [AAD Pod Identity](https://github.com/Azure/aad-pod-identity) in Azure or [IAM role for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) in AWS.\n\nWhen secrets in a pod are unavoidable, for example when a third party API key is needed, secrets should be injected into pods during deployment.\n\n**Note** the precise mechanism for managing this is still being reviewed with in a collaboration between Cloud Technology Working Group (CTWG), Future Farming and Countryside Programme (FFC) and Europe and Trade Delivery Portfolio (EuTDP). \n\nThis document will be updated when an agreement is reached.\n\nThe Kubernetes `Secrets` resource type must not be used as data is only Base64 encrypted.\n\n### Labels\nLabels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. \n\nLabels can be used to organise and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.\n\nIn order to take full advantage of using labels, they should be applied on every resource object within a Helm chart. i.e. all deployments, services, ingresses etc.\n\n#### Required labels\nEach Helm chart templated resource should have the below labels. Example placeholders are provided for values.\n\n```\nmetadata:\n  labels:\n    app: {{ quote .Values.namespace }}\n    app.kubernetes.io/name: {{ quote .Values.name }}\n    app.kubernetes.io/instance: {{ .Release.Name }}\n    app.kubernetes.io/version: {{ quote .Values.labels.version }}\n    app.kubernetes.io/component: {{ quote .Values.labels.component }}\n    app.kubernetes.io/part-of: {{ quote .Values.namespace }}\n    app.kubernetes.io/managed-by: {{ .Release.Service }}\n    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\n    environment: {{ quote .Values.environment }}\n```\n\n**Note** `Deployment` resource objects should have two sets of labels, one for the actual deployment and another for the pod template the deployment manages.\n\n### Selectors\nServices selectors should be matched by app and name. Selectors should be consistent otherwise updates to Helm charts will be rejected.\n```\nselector:\n  app: {{ quote .Values.name }}\n  app.kubernetes.io/name: {{ quote .Values.name }}\n```\n\n### Resource usage\nPredictable demands are important to help Kubernetes find the right place for a pod within a cluster. When it comes to resources such as a CPU and memory, understanding the resource needs of a pod will help Kubernetes allocate the pod to the most appropriate node and generally improve the stability of the cluster.\n\n#### Declaring a profile\n- all pods declare both a `request` and `limit` value for CPU and memory\n- production clusters do not contain any `best-effort` pods\n- pods with consistent usage patterns are run as`guaranteed` pods (i.e. equal `request` and `limit values`)\n- pods with spiky usage patterns can be run as `burstable` but effort should be taken to understand why performance is not consistent and whether the service is doing too much\n\n#### Resource profiling\nPerformance testing a pod is the only way to understand it's resource utilisation pattern and needs. Performance testing should take place on all pods to accurately understand their usage before they can be deployed to production.\n\n### Resource quotas\nClusters will limit available resources within a namespace using a `resourceQuota` to improve cluster stability. \n\nAn example `ResourceQuota` definition is included below\n\n```\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: my-resource-quota\nspec:\n  hard:\n    requests.cpu: \"1\"\n    requests.memory: 1Gi\n    limits.cpu: \"2\"\n    limits.memory: 2Gi\n```\n\n### Pod priority\nKubernetes Pods can have priority levels. Priority indicates the importance of a pod relative to other pods. If a pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority pods to make scheduling of the pending pod possible.\n\nIn the event of over utilisation of a cluster, Kubernetes will start to kill lower priorty pods first to maintain stability.\n\nClusters should include pod priority classes that teams can consume based on their service needs.  The below gives examples of the Pod Priorty classes available in FFC clusters.\n\n#### High (1000) \nReserved primarily for customer facing or critical workload pods.\n\n#### Default (600)\nDefault option suitable for most pods.\n\n#### Low (200)\nFor pods where downtime is more tolerable.\n\nBelow is a full example of a `priorityClass` resource definition for reference.\n\n```\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: default\nvalue: 600\nglobalDefault: true\ndescription: \"This priority class should be used for most services\"\n```\n\n#### Resource profile impact\nIn the event a cluster has to make a choice between killing one of two services sharing the same priority level, the resource profile configuration will influence which is killed.\n\n### Probes\nTo increase the stability and predicatability of a Kubernetes cluster, services should make use of both readiness and liveness probes unless there is a significant reason not to.\n\nProbe end points should follow the convention of `healthy` for readiness probes and `healthz` for liveness probes.\n",
      "repository_path": "docs/standards/kubernetes_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:40.580000",
      "updated_at": "2025-01-24T11:12:40.580000"
    },
    {
      "_id": "679375a9dd8933f95f8d8f10",
      "text": "\ufeff# PL/SQL Coding Standards\n\nThese standards have been derived from hands-on experience at the Environment Agency and Rural Payments Agency over many years. As such they are mature, comprehensive while also being reasonably lightweight and follow standard industry practice.\n\n## Structure\n\n> Use `Block` and `Label` coding for loops and anonymous blocks within procedures\n\n```sql\n CREATE OR REPLACE PROCEDURE my_proc\n\n IS\n\n BEGIN\n\n    <<Block1>>\n\n    DECLARE\n\n       -- statements\n\n       <<yearly_analysis>\n\n       FOR y_count IN yearly_analysis LOOP\n\n          <<monthly_analysis>>\n\n          FOR m_count IN monthly_analysis LOOP\n\n             -- statements\n\n          END LOOP monthly_analysis;\n\n          -- statements\n\n       END LOOP yearly_analysis;\n\n    END Block1\n\n END my_proc;\n```\n\n- Use a single `EXIT` point for loops\n\n- Use a single `RETURN` point for Functions\n\n## Declarations and Types\n\n- Anchor variables to Database data types\n\nThat way if table/column changes are made, the code does not to be updated to reflect the changes: Variables should never be designated as a standard data type (e.g. `VARCHAR2 (200)`, `NUMBER (10)`, etc.\n\n- Use `SUBTYPE` to standardise application-specific data types\n\nThis allows creation of aliases for existing data types that cannot be anchored to the database. These should be declared in a separate package, for example:\n\n```sql\n CREATE OR REPLACE PACKAGE my_vars\n\n IS\n\n    SUBTYPE counter IS INTEGER (10);**\n```\n\nThen in the DECLARATION section of the code:\n\n```sql\n DECLARE\n\n    v_counter my_vars.counter;\n```\n\n**This is the only place in the code where a hard-coded value is used.**\n\n- Use CONSTANT declaration for variables that do not change, and declare these in a CONSTANTS package\n\n```sql\n DECLARE OR REPLACE PACKAGE my_constants\n\n IS\n\n    C_max_hours_per_day CONSTANT NUMBER (2) := 24;\n```\n\nThen in the declaration section of a procedure\n\n```sql\n DECLARE\n\n    V_full_day my_constants.c_max_hoursper_day;\n```\n\n- Global variables should never be more than \"package-wide\"\n\n## Error Handling\n\n- All Anonymous blocks within a procedure should have their own exception handler in addition to the exception handler in the parent block\n\n```sql\n CREATE OR REPLACE PROCEDURE my_proc\n\n IS\n\n BEGIN\n\n    BEGIN\n\n       DECLARE\n\n       BEGIN\n\n       EXCEPTION\n\n       END;\n\n    EXCEPTION\n\n    END;\n\n EXCEPTION\n\n END my_proc;\n```\n\n- When performing bulk data operations (usually as some form of \"batch\" job) all tables should have a corresponding error table and all bulk update/insert/delete/merge statements should be suffixed with a LOG ERRORS statement referencing the errors table to prevent simple data errors terminating program execution\n\n- A standard error procedure should be used that records, in addition to user-defined information such as table name, `DBMS_UTILITY.FORMAT_CALL_STACK`, `DBMS_UTILITY.FORMAT_ERROR_STACK` and `DBMS_UTILITY.FORMAT_ERROR_BACKTRACE`\n\n## Layout\n\nPL/SQL is often written using a tool called [Toad](https://en.wikipedia.org/wiki/Toad_(software)) in Defra. It includes the ability to automatically format code using imported `.opt` files.\n\nWe have [guidance on how to do this](../guides/plsql_auto_format_toad.md), and [defra_plsql_toad_fmt.opt](../guides/defra_plsql_toad_fmt.opt) that applies the layout conventions described below.\n\n- Indents should be 3 characters wide. All code within loops and if statements should be so indented with one such indent for each level of nesting within if statements or loops\n\n- `THEN` should be positioned on the next line, in-line with the related `IF`\n\n- `LOOP` should be positioned on the next line, in-line with the associated `DO` or `WHILE`\n\n- `AS` or `IS` should be positioned on the next line, in-line with the associated statement (such as with an associated `CREATE`)\n\nLayout examples:\n\n```sql\n BEGIN\n\n    a := 1;\n\n    d := 1;\n\n    b := 2;\n\n    WHILE TRUE\n\n    LOOP\n\n       a := 1;\n\n       b := 2;\n\n       IF a > b\n\n       THEN\n\n          c := d;\n\n       END IF;\n\n    END LOOP;\n\n    x := 1;\n\n    c := d;\n\n END;\n\n CREATE PROCEDURE myproc\n\n AS\n\n BEGIN\n\n    NULL;\n\n END;\n```\n\n## Case and Lists\n\n- Oracle key words should be in uppercase\n\n```sql\n SELECT aaa\n\n , bbb\n\n , cccc\n\n , SIN (x)\n\n FROM mytab;\n```\n\n- Where you have lists, e.g. a list of column names or values, place each item on its own line, with commas, where needed, placed at the start of the line with a following space\n\nThis makes it far easier to make changes, such as inserting new items into the list, without losing track of the commas.\n\n```sql\n CREATE TABLE my\\_table1\n\n (\n\n my\\_num1 NUMBER\n\n , my\\_num2 NUMBER\n\n );\n\n INSERT INTO building\\_blocks (aaaaaaaaaaaaaaaa\n\n , bbbbbbbbbbbbbb\n\n , cccccccccccccccc\n\n , ddddddddddddddddd\n\n , eeeeeeeeeeeee)\n\n SELECT aaaaaaaaaaaaaaaa\n\n , bbbbbbbbbbbbbbbbbbb\n\n , cccccccccccccccccc\n\n , dddddddddddddddd\n\n , eeeeeeeeeeeeeee\n\n , fffffffffffffffff\n\n FROM mytab, histab, hertab\n\n WHERE histab.col1 = hertab.col2;\n```\n",
      "repository_path": "docs/standards/plsql_coding_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:41.897000",
      "updated_at": "2025-01-24T11:12:41.897000"
    },
    {
      "_id": "679375aadd8933f95f8d8f11",
      "text": "\ufeff# Java Coding Standards\n\nThese standards are a slightly updated version of the standards used in practice for many years in the Environment Agency during the period when Java was defined as the strategic programming language. As such they are comprehensive, mature and follow standard industry practice.\n\nFor users of [Eclipse](https://en.wikipedia.org/wiki/Eclipse_(software)) we have [guidance on how to auto-format your code](../guides/defra_java_eclipse_code_style.xml) that applies the standards described below.\n\n## Naming, Structure and Declarations\n\n- English descriptors should be used for all variables and classes such that they define the purpose of the object they describe\n\nThe only allowable exceptions to this rule are firstly where local variables are used as iteration counters where `i` is the accepted standard. Where multiple counters are required they should start alphabetically at `i` then `j` etc. Secondly exceptions may be abbreviated for example in the catch clause of a programmatic block an exception may be called `e`.\n\n- All variables should start with a lower case letter and the initial of any non-leading word should be capitalised\n\nFor example `fishingLicense`, `dataElement`. This rule should ensure that variable names do not differ only in case. One exception to this rule is where a name includes a word which is commonly capitalised such as an acronym. An example of this would be `userID` or `serialUID`.\n\n- Do not use leading or trailing underscores\n\n- Local variables should not share the same name as a variable with greater scope\n\nThe only exception to this is in a setter, where the passed in parameter name should match the name of the instance variable.\n\n- Class and interface names should begin with a capital letter and the initial of any non-leading word should be capitalised\n\nFor example `FishingLicenceComparator`, `ValueObjectEngineer`.\n\n- Static final variables should be named with capitalised names and should have words separated by an underscore\n\n```java\nSALMON_LICENCE_TYPE\n```\n\n-  Arrays should be declared using the bracket notation after the datatype not the variable\n\n```java\nbyte[] fileBytes\n```\n\n- Where methods are overloaded they should appear next to each other in the class\n\n- Objects should be initialised to `null` at declaration time\n\n- Accessor and Mutator methods (gets and sets) should follow the Javabeans naming conventions\n\nFor example the variable `licenceCount` would be accessed by the method `getLicenceCount()`.\n\n- All classes and interfaces should belong to an explicit package\n\n- All packages should be prefixed by `uk.gov.defra`. Package identifiers should all be singular i.e. should not contain plurals\n\n- Packages should not be imported in their entirety by using the wildcard notation\n\nIf using an IDE such as Eclipse this, plus removing any unwanted imports, ought to be the default behaviour of the IDE.\n\n- Where possible methods and variables should be referenced without using full name qualification\n\nFor example reference `IOException` rather than `java.util.IOException`.\n\n- Constructors should appear as the first methods in any class, followed by the finalize() method if overridden, followed by static methods and then any other methods and finally any inner classes\n\n## Statements and Expressions\n\n- A single line should not contain multiple statements\n\n- When using unary operators no space should be left between the variable and the operator\n\n```java\nLicences++;\n```\n\n- For readability, equality operator must always be wrapped in spaces\n\n```java\nlicenceType == 2\n```\n\n- For readability assignment operators must be wrapped in spaces\n\n```java\nLicenceType = 3;\n```\n\n- For readability, arrow (lambda) operators must be wrapped in spaces\n\n```java\ninterface NumericTest\n{\n  boolean computeTest(int n);\n}\n\nNumericTest isNegative = (n) -> (n < 0);\n```\n\n- Logic and binary operators should be wrapped in spaces\n\n```java\npayAward += 4;\n\n(payAward >= 1000);\n```\n\n- The semicolon closing a statement should immediately follow the statement and should not be preceded by a space\n\n- A single blank line should appear between a method end and the next method's leading comments\n\n- Indents should be 2 characters wide\n\n- Opening and closing braces should be aligned and not appear on the same line\n\n```java\ntry\n{\n  // some actions\n}\ncatch (Exception e)\n{\n  // handle exception\n}\n```\n\n- Lambda expressions should ideally be single line, in which case the entire expression should wherever possible be kept to the same line (i.e. unless forced to wrap as the line exceeds the maximum specified line length).\n\nFor example, assuming this functional interface:\n\n```java\ninterface NumericTest\n{\n  boolean computeTest(int n);\n}\n```\n\nA correctly formatted lambda expression could look like:\n\n```java\nNumericTest isNegative = (n) -> (n < 0);\n```\n\n- Where a lambda expression spans many lines, the braces surrounding those lines should follow the same rules as all other braces\n\nFor example, assuming this functional interface:\n\n```java\ninterface NumericTest\n{\n  boolean computeTest(int n);\n}\n```\n\nA correctly formatted lambda expression could look like:\n\n```java\nNumericTest isNotEven = (n) ->\n{\n  n++;\n  return n % 2 == 0;\n};\n```\n\n## Loops and Conditionals\n\n- Loops and conditionals should be blocked out in the following manner unless the content is a single line statement\n\n```java\nfor (int i=0; i<maxValue; i++)\n{\n  // loop body goes in here\n}\n\nif (licenceType == 1)\n{\n  // conditional body goes in here\n}\n\nswitch (licenceCode)\n{\n  case F172:\n    // do actions\n    break;\n\n  default:\n    // do actions\n    break;\n}\n```\n\nFor single line statements either the above approach or the following is permissible.\n\n```java\nfor (i = 0; i <= 12; i++)\n\nj = j + i;\n```\n\nThe important thing is that the content of the loop is clearly distinguished, either by the braces or, in the case of a single line and only a single line, that line being separately indented.\n\n- Indentation should be used to indicate enclosed statements in loops and conditionals\n\n## Class, Interface and Method Declaration\n\n- Classes and interfaces should be declared, if possible, on a single line. Where this is not possible indentation should be used to aid the reading of the class declaration as follows\n\n```java\npublic class MySpecialisedClass extends MyGenericClass\n implements Serializable\n{\n  // class body goes in here\n}\n```\n\n- Methods should be declared, if possible, on a single line\n\nWhere this is not possible indentation should be used to aid the reading of the method declaration as follows.\n\n```JAVA\npublic myIOMethod(String param1, Integer param2) throws IOException,\n  FileNotFoundException\n{\n  // method body goes in here\n}\n```\n\n## Commenting and Documentation\n\nSee the [Common coding standards](common_coding_standards.md) for the general standards and guidance on commenting code.\n\n### JavaDoc\n\n- All classes and interfaces should include JavaDocs comments as a header\n\nThe comments should explain in plain English what the purpose of the class or interface is, where that purpose is not obvious from the name. The JavaDoc header should be placed below the package name and import list and above the class definition.\n\n- The following header format should be used\n\n```java\n/**\n*  Original Author: @author\n*/\n\n/**\n*     Brief description of what MyClass does.\n*\n*/\n```\n\nNote that any information about the class history, beyond the identity of the original author, is not included here, as such information should be included in the version control system. Attempting to maintain such information in the class comments themselves is not only duplicate effort but is likely to result in the two sources of information becoming confusingly inconsistent.\n\n- All methods should be preceded by JavaDoc comments\n\nComments must state in plain English what the purpose of the method is, what effect it has on any referenced objects and any pre or post conditions that apply to the method's use. The comment should also indicate if the method overrides an inherited method. The javadoc should include entries for `@param`, `@return` and `@exception`, wherever those are appropriate.\n\n```java\n/** Method To return a count of the outstations associated with the location\n*    @param locID, the locationID to search on\n*    @return Integer, number of outstations, null if none are found\n*    @exception DataAccessException, thrown when outstations can't be counted\n*/\n```\n\nOther javadocs tags can be used as required.\n\n### Other Comments\n\n- Both multi-line (`\\* */`) and single line (`//`) comments may be used\n\n- Comments within the method body should, wherever possible, be restricted to single line comments\n\n## Status\n\nThis standard was formally adopted on 20 Feb 2021.\n",
      "repository_path": "docs/standards/java_coding_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [
        "67935eda361ba58e4bb051aa"
      ],
      "created_at": "2025-01-24T11:12:42.936000",
      "updated_at": "2025-01-24T11:12:42.936000"
    },
    {
      "_id": "679375acdd8933f95f8d8f12",
      "text": "\ufeff# Uipath Best Practices\n\n\n## Developer Best Practices\n\n### Do\n\n#### Check if a library component already exists for your need before you start development. If you find a component that almost meets the requirement amend the component as required.\nThis helps save time, prevents duplication of components, and helps keep the components library up to date.\n\n#### If your code has a loop ensure there is an infinite loop failsafe within it.\nE.g. you can build in a counter in your loop to keep track of how many loops have taken place. You can then throw an exception or escape the loop at this point. This will prevent the automation from getting stuck.\n\n#### If users will have sight of exception messages then try catch specific errors and reword them to be more user friendly.\nThis may reduce the number of MyIT tickets raised for our team. If possible add in a solution to the error message. E.g. \u201cThe Excel file is already open. Please close the file then restart the robot.\u201d\n\n#### Please follow the 3/4 letter prefix and Hungarian case naming conventions.\nE.g. strStringVariable, boolBooleanVariable, arrArrayVariable, dictDictionaryVariable. Give variables descriptive names. This ensures the code is easier to navigate and read.\n\n#### Prefix all arguments with in, out, or io.\nE.g. io_dictConfig\n\n#### Use the 64-bit Template with modern design experience.\nSome applications may not be compatible with 64-bit framework \u2013 in which case there is a 32-bit template available.\n\n#### Annotate complicated code or any code requiring additional information.\nComment activity is also available to add text to.\n\n#### Ensure activities have descriptive names.\nE.g. Assign \u2013 Initialise Dictionary\n\n### Documentation\n\n#### When working on a new application please ensure that you grant the same level of access to all team members.\nThis helps reallocate work in case additional support work is required or there is unplanned leave.\n\n#### Each project/work package should have its own repository. Naming convention to be used \u2013 [Agency][Workpackage Name] i.e. EA-Waste-Tonnage-Returns\nEmail Ben Sagar to create the repository.\n\n#### Upon completion of a project \u2013 please book a technical video handover with 2 Developers.\nPlease upload this to our SharePoint Handover Videos. Remember to add the link URL into the Technical Developer document.\n\n#### Convert useful code into library components.\nYou may create some useful reusable code for a new project. Please try to convert it into a library and book a share and learn with the team.\n\n#### Create guides and add them to Guides.\nYou may have to do some configuration or set up a new application as part of a project or be given some RnD to complete. Please create a step-by-step guide with screenshots for anything that will be useful for the development team.\n\n#### Work from the OneDrive folder and provide the development team with access to that folder.\nWhere a developer is on leave and may not have Git pushed this helps the development team save time by retrieving code unavailable in GitHub.\n\n#### Use Orchestrator Assets for configurable input data.\nThe exception being if there is a project specific reason e.g. sensitive data requiring you to use a custom approach.\n\n#### Complete the Developer technical document as and when you can.\nFilling in the document throughout the development process will ensure nothing is forgotten or missed.\n\n### Try\n\n#### Try using simulate click/background activities over foreground versions.\nFor attended mode it allows users to use mouse & keyboard while a robot is running. This approach is faster and more stable than foreground counterparts.\n\n#### Consider when you should use a sequence, flowchart, or a state machine.\nSometimes switching your code to a different format can improve readability. E.g. the \u2018if activity\u2019 and \u2018flow decision\u2019 activity are both if statements. However swapping \u2018flow decisions\u2019 with \u2018if statements\u2019 or vice versa can make an impactful change.\n\n#### Using a template for your project.\nTemplates are available and kept up to date by the development team. There may be a rare scenario requiring you to create custom code. Please check with a Lead developer if the template is not fit for purpose before you create custom code.\n\n#### If you must use foreground activities on an unstable selector ensure there is confirmation of code execution.\nWrap the foreground activity with a retry scope and some verification. Alternatively use the modern design mode built-in verification.\n\n#### For message boxes addressed to the user please make use of the popup\u2019s library components.\nThis is to ensure consistency in our department branding. Where you create your own try and keep the theme consistent with our branding.\n\n#### Use the latest dependency.\nLatest dependencies \u201cusually\u201d are faster and more stable.\n\n#### Git push at the end of the day or after a meaningful change.\nThis helps to version control or checkpoint allowing you to return to a previous position. It further allows others to download and distinguish between completed code and partially built code.\n\n#### Maintain consistency in your coding/readability.\nIf you discover an improved method as you are developing a project please ensure your entire code is in line with it for a consistent approach.\n\n#### Asynchronous approach. Parallel coding style. Check outcomes simultaneously if possible.\nTry using pick branches when you review multiple outcomes this is faster than having a chain of if activities. The state machine also has a built-in pick machine in the transition lines. *Please note this is not truly asynchronous to achieve asynchronous code you can make use of invoke process.* The pick branch executes the triggers in sequence but doesn\u2019t wait for the activities to finish before executing the next trigger (making it seem like parallel or asynchronous).\n\n#### Stay up to date with UiPath software.\nThis can be done by reading the latest release notes and watching the latest UiPath training academy videos. Share findings with the development team through share and learns.\n\n### Avoid\n\n#### Hardcoding values especially usernames/passwords\nThis is required as you may upload usernames/passwords to a public GitHub repository. It will be easier to change the values of any configurable from outside the code. E.g. Assets\n\n#### Image based automation\nKeep this as a last resort - image based activities are slow and unstable.\n\n#### Nesting activities\nE.g. an if within an if within \u2026 Nesting activities within each other makes the code difficult to read. Try to change your approach to reduce the level of nesting. E.g. use a switch case instead of a nest of ifs.\n\n#### Spaghetti code\nSpaghetti code refers to a difficult to follow coding style. Your code should be easy to follow in a flowchart. E.g. Having crossed lines in a flow chart will make it difficult to follow. E.g. Splitting a function written in several different invokes.\n\n#### Storing passwords/sensitive information in Orchestrator\nSome of our stakeholders do not authorise the use of cloud services. Please check with stakeholders for the preferred method of handling data.\n\n#### Leaving developer code in final product main files\nIt is acceptable to leave separate testing xaml\u2019s however avoid leaving commented out code in the main code or temporary write-lines etc\u2026\n\n#### Exception swallowing\nIf you use a try catch ensure exceptions are logged as a minimum because an empty catch makes it difficult to pinpoint the root cause during debugging.\n\n#### Complicated/complex coding techniques\nAvoid using complex coding techniques that could be done with simpler methods. If complex code is required please include an explanation within the annotation of the code. E.g. Using invoke code with vb.net instead of using predefined activities.\n\n#### Complicated variables\nE.g. Use a datatable/json object instead of dictionary within a dictionary variable.\n\n#### Lasagne code\nIt is good practice to break up a flow into smaller flows however if this is done frequently the code becomes too layered. Try to keep how deep invokes within invoke layers reach. In addition don\u2019t use one xaml file for everything! Avoid the use of single activity invokes or single activity components.\n\n#### High number of variables\nTry to reduce the number of variables used. It is difficult to track information being transferred during debugging if it\u2019s passed from variable to variable. Try to keep arguments and variables that connect to each other in different xamls similarly named. You could make use of global variables if possible. Add into config variable if you need to use variable everywhere.\n\n#### Using delay activity\nTry to keep the delay activity as a last resort. You should be able to use find element as a wait which will stop waiting as soon as confirmation is received e.g. click followed by a find element. Consider also reducing the delays in the activities to speed up your code.\n\n## Status\nThis revised standard was formally adopted on 23 July 2024\n\n## Significant changes\nOriginal version adopted October 2020.\\\nRe-write July 2024 based on the revised ways of working implemented from 2021 onwards.",
      "repository_path": "docs/standards/uipath_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:44.024000",
      "updated_at": "2025-01-24T11:12:44.024000"
    },
    {
      "_id": "679375addd8933f95f8d8f13",
      "text": "# Container standards\n\n## Terminology\n`Dockerfile` - set of instructions for building a docker image  \n`Image` - a constructed set of layered docker instructions  \n`Container` - a running instance of an image\n\n## Standards\n## Defra base images\nImages are extended from a minimal Defra created parent image.  This allow us to benefit from improved security and more efficient builds as we will not have to repeat steps that are common to all Dockerfiles.\n\nDifferent parent images should be created in line with each framework's best practice.  Eg, Node.js, .Net Core, Ruby etc.\n\nDefra's Dockerfiles to build parent images can be found in the below GitHub repositories:\n- [Node.js](https://github.com/DEFRA/defra-docker-node)\n- [.Net Core](https://github.com/DEFRA/defra-docker-dotnetcore)\n\nEach image is hosted in DockerHub and has an equivalent development and production image.  \n\n- [Node.js](https://hub.docker.com/repository/docker/defradigital/node)\n- [Node.js development](https://hub.docker.com/repository/docker/defradigital/node-development)\n- [.Net Core](https://hub.docker.com/repository/docker/defradigital/dotnetcore)\n- [.Net Core development](https://hub.docker.com/repository/docker/defradigital/dotnetcore-development)\n\n### Images are created using Docker\nDocker is the de facto industry standard in most enterprises.\n\n### Docker Compose is used for defining container builds\nUsing a consistent containerisation tool will help enforce standards and better promote developer mobility.\n \n### Linux containers are used if the service to be containerised can run on Linux\nUnderlying container hosts can only host either Linux or Windows containers.  A model of consistency would reduce the number of underlying hosts.  Windows containers can only be run on Windows hosts which typically have greater licensing costs vs Linux.\n\n*Note that services built using .Net Framework cannot be run using Linux containers, but services built using .Net Core can.  Defra's position is .Net Core should be preferred over .Net Framework but there may be some use cases if containerising legacy services*\n\n### Images are built using Defra base images (see above)\n### Containers are not run using root user\nContainers are not trust boundaries and therefore should never be run as root for security reasons.\n\n### Public images are signed\nA digital fingerprint should be added to each image.  This enables consumers of images to verify the source and trust levels of the image they are consuming.\n\n### Images are tagged using semantic versioning\n[Version control standards](version_control_standards.md)  \n\nFor simplicity, it is beneficial to keep the image and containerised application version in sync.\n\n### Production images should be immutable and once built do not change but can be configured\nImmutable images do not change once created which gives the a versioned image predictability and portability.\n\n### Images are self contained and carry all runtime dependencies\nImages should not be dependent on their host infrastructure for any application dependency.  This enables them to be freely deployed and orchestrated anywhere capable of running containers.\n",
      "repository_path": "docs/standards/container_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [
        "6790cb8170024c99a94725a5",
        "678fcc01c24985192f15edc6",
        "678fcc2bc24985192f15edcc"
      ],
      "created_at": "2025-01-24T11:12:45.652000",
      "updated_at": "2025-01-24T11:12:45.652000"
    },
    {
      "_id": "679375aedd8933f95f8d8f14",
      "text": "# Quality assurance and test standards\n\nThis page covers the minimum standards that apply to quality assurance (QA) and testing on Defra's digital services.\n\nAll digital services must meet the [GOV.UK service standard](https://www.gov.uk/service-manual/service-standard).\n\nGovernment Digital Service guidance on testing services is available on the [technology page of the service manual](https://www.gov.uk/service-manual/technology).\n\nFurther QA & Test guidance is available on the [Defra QA & Test wiki](https://github.com/DEFRA/qa-test/wiki).\n\n## Acceptance criteria\n\nWherever possible, define positive and negative acceptance criteria up front, **before** a story is developed.\n\nDefault acceptance criteria for new stories:\n\n* Screens, behaviour and content match designs from the prototype or wireframe\n* Styles match the [design system](https://design-system.service.gov.uk/)\n* Accessible - meets [Web Content Accessibility Guidelines (WCAG) version 2.1](https://www.w3.org/WAI/standards-guidelines/wcag/glance/) at levels A and AA\n* Works across [all supported browsers](https://www.gov.uk/service-manual/technology/designing-for-different-browsers-and-devices) including IE11 and mobile\n* Server-side [error validation](https://design-system.service.gov.uk/components/error-message/) exists for all fields\n* No obvious performance issues (most transactions under 1 second, avoid transactions over 10 seconds)\n* No existing functionality has regressed\n\nNegative scenarios are just as important as positive ones. For example, \"an admin user can export data\" implies that \"a standard user cannot export data\". Routinely check errors in manual and automated tests.\n\n## Accessibility\n\nAll digital services, internal or external-facing, must meet [Web Content Accessibility Guidelines (WCAG) version 2.1](https://www.w3.org/WAI/standards-guidelines/wcag/glance/) at levels A and AA.\n\n[Accessibility information, guides and checklist](https://github.com/DEFRA/qa-test/wiki/Accessibility) used in Defra\n\n[GOV.UK accessibility guidance](https://www.gov.uk/service-manual/helping-people-to-use-your-service/making-your-service-accessible-an-introduction)\n\n## Automated testing\n\nThe default test technologies for new projects are:\n\n* Cucumber\n* [Webdriver.io](https://webdriver.io/)\n* Node.JS\n* Selenium\n\n[More test automation information and guides](https://github.com/DEFRA/qa-test/wiki/Automation)\n\nUse GitHub to publish test code in the open, but **never** publish any passwords or other sensitive data. Passwords can be stored in private files or repositories (for example hiding a file using `.gitignore`), or via local environment variables.\n\n## Browser testing\n\nTest services on the [GOV.UK set of required browsers](https://www.gov.uk/service-manual/technology/designing-for-different-browsers-and-devices).\n\n## Bug (defect) management\n\nBy default, use [Jira](https://www.atlassian.com/software/jira) to manage bugs as follows:\n\n* If a bug is as a direct result of a story that's currently in development, then attach the details to that story.\n* If a bug is a regression issue then raise a separate bug in Jira.\n\n[Defra Jira guidance and list of administrators](https://github.com/DEFRA/qa-test/wiki/Jira)\n\n## Code quality testing\n\nAutomated test code must meet Defra [common coding standards](https://github.com/DEFRA/software-development-standards/blob/master/standards/common_coding_standards.md) wherever applicable.\n\nAll development and test code must undergo regular linting to check that it meets minimum standards.\n\n## Compliance checking\n\nEnsure your service's compliance with the following:\n\n* [Accessibility statements](https://www.gov.uk/guidance/make-your-website-or-app-accessible-and-publish-an-accessibility-statement)\n* [Assisted digital support](https://www.gov.uk/service-manual/helping-people-to-use-your-service/assisted-digital-support-introduction)\n* [Cookie policies](https://www.gov.uk/service-manual/technology/working-with-cookies-and-similar-technologies)\n* [General Data Protection Regulations (GDPR)](https://www.gov.uk/government/publications/guide-to-the-general-data-protection-regulation)\n* [GOV.UK content styles](https://www.gov.uk/guidance/content-design/writing-for-gov-uk)\n* [GOV.UK design system](https://design-system.service.gov.uk/)\n* [Privacy policies](https://www.gov.uk/service-manual/design/collecting-personal-information-from-users) - especially when communicating with third parties such as Google for analytics\n* [Progressive enhancement](https://www.gov.uk/service-manual/technology/using-progressive-enhancement), in particular that your service is usable without Cascading Style Sheets (CSS) and JavaScript\n\n## Continuous integration (pipeline)\n\nBy default we use Jenkins for continuous integration and to manage deployments.\n\nWhere possible, build automated tests into the team's continuous integration process, so that only working code can be deployed. These can be done for functional testing as well as performance and accessibility.\n\n## Documentation\n\nQuality assurance must produce the following documentation as part of any major work:\n\n* **Test plan**, using the headings on this page as a guide\n* **Test completion report** before a major release, detailing the status of any testing and bugs, and any outstanding risks\n* **Automated test suite** with features written in a format readable to non-technical colleagues\n\nOfficial templates are on the QA & Test Sharepoint although these can be adapted to meet the needs of the team.\n\nFor each user story, document its testing alongside that story - for example, as Jira comments or a linked document.\n\nOther documentation, produced by QA as required:\n\n* **QA information document** covering practical information such as how to access test environments and data.\n* **Site map** - a list of pages on the service and whether they are covered by automated tests\n* **Performance test report**\n\n## Exploratory testing\n\nAll stories should undergo a level of manual, exploratory testing based on the perceived level of risk that needs mitigating. This relies on the experience of the tester. If unsure how best to approach this, the speak to a QA colleague to help put together an exploratory test charter.\n\n[GOV.UK information on exploratory testing](https://www.gov.uk/service-manual/technology/exploratory-testing)\n\n## High availability testing\n\nBefore releasing a new production environment, work with the WebOps team to test and monitor how the service recovers from servers being deactivated.\n\n## Penetration (security) testing\n\n[GOV.UK advice on vulnerability and penetration testing](https://www.gov.uk/service-manual/technology/vulnerability-and-penetration-testing)\n\n## Performance testing\n\nThe default tool in Defra is [JMeter](https://jmeter.apache.org/).\n\nAny performance test should include the following by default:\n\n* **Load** - can the service deal with 120% of the likely maximum load?\n* **Soak** - can the service deal with high load for a prolonged period without memory leakage?\n* **Stress** - what is the service's breaking point, and how does it recover?\n\n## Reporting\n\nThe minimum standards for agile test reporting are to:\n\n* ensure the team is aware of major testing risks, for example through daily stand ups\n* produce a test completion report before a major release.\n\nHowever, certain key metrics are recommended to get a picture of quality over time, such as:\n\n* list of any gaps in testing and associated risks\n* number of outstanding bugs by priority\n* RAG (red/amber/green) status against each test type in the test plan\n* unit test coverage % (via code coverage tools)\n* user interface (UI) test coverage % (for example, by % of screens with test coverage)\n* user satisfaction % (if already live)\n\n## Risk management\n\nShare any risks that originate from QA activity with your product manager and ensure that you have sight of the team risk register.\n\n## Unit and API testing\n\nMaintain visibility of the unit and API test approach, and test coverage, such as via code quality metrics.\n\nAs a team, adopt the testing pyramid: aim to find the most defects at the lowest levels (unit and API) and fewer defects at the levels where it's slower to test (automated and manual user interface tests).\n\n## User needs and usability\n\nEnsure that all features map back to user needs by joining the team's user research and usability testing. The recommendation is to be involved for [2 hours every 6 weeks](https://www.gov.uk/service-manual/user-research/how-user-research-improves-service-design).\n\n[Test your service with users with disabilities](https://www.gov.uk/service-manual/user-research/running-research-sessions-with-people-with-disabilities).\n",
      "repository_path": "docs/standards/quality_assurance_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:46.758000",
      "updated_at": "2025-01-24T11:12:46.758000"
    },
    {
      "_id": "679375b0dd8933f95f8d8f15",
      "text": "# Common coding standards\n\nThese are common standards that apply to coding.\n\nMostly these follow common [programming principles](https://en.wikipedia.org/wiki/Category:Programming_principles)\n\n## Rationale\n\n- Ensure consistency across coding standards for different languages\n\n## Standards\n\n### All code is open\n\nWhilst we acknowledge that there are cases where it may be necessary for code to not be open, this standard ensures that those cases are managed as exceptions.\n\n### All coding is done on a branch\n\nNo code changes are made directly to the master branch.\n\nThis reflects the 'golden' state of the branch and mirrors the general standard that changes are never made directly to production services.\n\n### All code is checked for correct coding style\n\nLanguage-dependent\n\n### All branches have CI\n\nCovering code style, security checking, dependency checking, build and unit tests.\n\n### Unit test coverage is at least 90%\n\nAll changes must have sufficient coverage and the overall total coverage must not decrease.\n\n### All code is reviewed\n\nAll code changes are reviewed by another developer before they are merged.\n\n### All code is checked for readability\n\n> There are only two hard things in Computer Science: cache invalidation and naming things.\n>\n> -- Phil Karlton\n\nAvoid basic errors such as\n\n- Generic variable names: `p`, `temp`, `v`\n- Using arrays over hashes: `['Bob', 'Smith', 'Bristol']` over `{ name: 'Bob', surname: 'Smith', location: 'Bristol' }`\n- Use of acronyms: particularly domain specific ones. Someone new to the service may not know their GOR from their WIMS\n\nOther than that when considering the name for a method, class, module etc try not to approach it from what makes sense to you, but instead what best describes the intent of the thing, and what will help the person who comes after you.\n\nStick to the conventions and naming standards of the language or framework you are using.\n\nAim to write readable code that does not require comments.\n\nUse the principle of **separate in order to name**. Essentially break your code down into small (and if necessary one line) sub routines that have names that express what their intent is. Read [Write readable code without comments](http://www.wikihow.com/Write-Readable-Code-Without-Comments) for more on how to do this.\n\nIf you have to add comments, make them useful\n\n- **Comments are code** - Keep them small, simple and re-factor them as you make changes\n- **Don't document the _how_** - Document *why* the thing is needed, or *why* its doing it the way it is\n- **Comment in the right place** - Talk about the module at the module declaration, the subroutine at the method declaration etc\n- **Only write about the code** - Don't include names, dates, commit comments or your opinions\n\nDon't include commented out code. It's noise that only confuses. Use the commit history if you need to see what was there before.\n\nAnd most importantly write them *as you write the code*. If you don't, you probably never will!\n\n### All code is appropriately documented\n\nDocument the *how* for the project\n\nWhen it comes to other developers or users of the project, the thing they are most interested in is the *how*. *How* can I build/deploy/run this project. *How* can I use it in my own work.\n\n### All code is checked for simplicity\n\nWe don't code for reuse until we have a confirmed need for reuse.\n\nWe apply the [rule of three](https://en.wikipedia.org/wiki/Rule_of_three_(computer_programming)) to help decide when to abstract code.\n\nWe don't break up our applications until there is a confirmed need to do so.\n\nThe path to reuse starts with module/namespacing. When the need for reuse is confirmed we first build a package. Only if there is demand from multiple services, and the benefits for a deployed service over a shared package are clear do we build a new application.\n\n### All code is checked for quality\n\nAll repos are connected to a quality analysis tool and the tool's maximum quality rating is maintained.\n\nAll repos on GitHub and Azure Repos are connected to our SonarCloud organisation and the Defra standard quality gate is met.\n\n### All code is checked for security\n\nAll repos are connected to a security tool to monitor for vulnerabilities in the dependencies and static code vulnerabilities.\n\n### All code has dependency checking\n\nNo out of date dependencies or libraries with known vulnerabilities.\n\n## Status\n\nThis standard was formally adopted on 1 July 2019.\n\n## Significant changes\n\nSonarCloud was adopted as our standard quality analysis tool on 1 April 2020.\n",
      "repository_path": "docs/standards/common_coding_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:48.828000",
      "updated_at": "2025-01-24T11:12:48.828000"
    },
    {
      "_id": "679375b1dd8933f95f8d8f16",
      "text": "## Node.js standards\n\n### General\n- Node.js code is JavaScript code and should follow the [JavaScript standards](javascript_standards.md).\n- Don't use TypeScript.\n- Session state should not be stored on the node app server. Don't tie a session to a particular node server instance. Use a distributed cache or document storage database and not something like express-session. \n- Avoid blocking the [main event loop and the worker pool](https://nodejs.org/en/docs/guides/dont-block-the-event-loop/). In short \"you shouldn't do too much work for any client in any single callback or task.\" and consider passing CPU intensive tasks off to another service.\n- Prefer await over callbacks and avoid nested callbacks. This is easily done in [Node 8 and above](https://nodejs.org/api/util.html#util_util_promisify_original).\n\n### Versions\n\n- Be aware of the [Node.js support timeline](https://nodejs.org/en/about/releases).\n- Keep on Active LTS versions.\n- Don't drop behind Maintenance LTS versions. Projects older than this will be considered unmaintainable and\n  unsupportable until brought up to an appropriate version.\n- Don't progress beyond Active LTS versions.\n\n### Package Management\n- Use NPM.\n- Use a package.json and package-lock.json for repeatable builds.\n- Use an automated checker such as Snyk or Dependabot to ensure that your dependencies are up to date with the\n  latest patches.\n- Separate dependencies and dev dependencies.\n- Update your version number inline with the [semantic versioning standard](https://semver.org/).\n\n### Server framework\n- Our standard framework is [Hapi](https://hapijs.com/).\n- Be aware of the [Hapi support timeline](https://hapi.dev/support/#plans).\n- Keep on the current major Hapi version.\n- Don't drop behind the lowest version available through Hapi commercial support. Projects older than this will be\n  considered unmaintainable and unsupportable until brought up to an appropriate version.\n\n### CommonJS vs ES modules\n- ES modules should be used by default over CommonJS modules.\n- For scenarios where ES modules are not appropriate then CommonJS may be used.\n\n> Note: Some Node.js packages such as [Jest](https://jestjs.io/) are not fully compatible with ES modules and it may be more pragmatic to use CommonJS.\n\n### Status\n\nThis standard was formally adopted on 8 January 2020.\n\n### Significant changes\n\nClarification on preference between CommonJS and ESM added 29 July 2024.\n",
      "repository_path": "docs/standards/node_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [
        "678fcbe9c24985192f15edc2",
        "6790cb8170024c99a94725a5"
      ],
      "created_at": "2025-01-24T11:12:49.905000",
      "updated_at": "2025-01-24T11:12:49.905000"
    },
    {
      "_id": "679375b2dd8933f95f8d8f17",
      "text": "## JavaScript standards\n\nUse vanilla JavaScript, don't use extensions to the language such as TypeScript\n\n### Use [Standard JS](https://standardjs.com/) to lint your code\n\n- A consistent approach to code layout, spacing and formatting makes it easier to switch between projects.\n\nBy [adding it as a dev dependency](https://standardjs.com/index.html#install) it can be easily used in the terminal,\n[Webstorm](https://blog.jetbrains.com/webstorm/2017/04/using-javascript-standard-style/),\nand [Visual Studio](https://marketplace.visualstudio.com/items?itemName=chenxsan.vscode-standardjs).\n\n### Do not use front end JavaScript frameworks\n\n- Frameworks such as React, Angular, Vue etc go against [progressive enhancement](https://www.gov.uk/service-manual/technology/using-progressive-enhancement)\n- Even frameworks that provide server-side rendering encourage and enable accidental use of client-side approaches, which violate the progressive enhancement principle\n- The [GOV.UK Design System](https://design-system.service.gov.uk/) already provides a set of accessible components that support progressive enhancement\n\nA service could make a case for using a framework to deliver a highly-functional progressive enhancement, for example an interactive map, but these must be managed as explicit exceptions to this standard.\n\n## Status\n\nThis standard was formally adopted on 1 July 2019.\n\n## Significant changes\n\nClarification on not using front end frameworks was added 1 December 2024.\n",
      "repository_path": "docs/standards/javascript_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [
        "678fcbe9c24985192f15edc2"
      ],
      "created_at": "2025-01-24T11:12:50.951000",
      "updated_at": "2025-01-24T11:12:50.951000"
    },
    {
      "_id": "679375b4dd8933f95f8d8f18",
      "text": "# Mobile application standards\n\n## Introduction\n\nThe huge increase in the use of mobile devices means that mobile application development is now of critical interest to the Defra family, even more so given the amount of field-work being carried out by workers within the Defra family.\n\n## Scope\n\nThese standards apply to mobile application in-house development and management.\n\nThese standards are intended to cover all forms of mobile application.\n\nAs implied, there are a number of ways of delivering mobile apps. These standards will cover which approach is best and under what circumstances, and aspects that are common to all or many of the approaches. As and when more detail is available on specific approaches, such as Flutter app development, or Progressive Web Apps, such approach-specific standards should form part of their own separate, detailed, documented standard, which, ideally, will simply be a reference to a recognised external standard for that specific technology.\n\n## Caveats\n\nThis is a fast moving area involving multiple platforms and diverse technology stacks: these standards are therefore early and provisional. As such, these standards often require more background explanation and justification than those for other, more mature or more homogenous, technology stacks. Inevitably, therefore, these standards, for the time being, are likely to be more discursive than those for other areas, and, unavoidably, the line between standard and guidance can often, as yet, not be so sharply drawn.\n\nThis document should therefore be read in conjunction with the Mobile Application Guidance (**[guidance](../guides/mobile_app_guidance.md)**).\n\n## What is a mobile app?\n\n### Assume we are providing an offline mobile app unless we have clear agreement that the app will only work online\n\nMobile apps are commonly defined along the following lines: \"A mobile application, also referred to as a mobile app or simply an app, is a computer program or software application designed to run on a mobile device such as a phone, tablet, or watch.\"\n\nIn practice, the user need that often drives the requirement to have a mobile app in the first place is the necessity to use the app when the user is out and about, including in situations where there is no signal and therefore no internet access. Indeed, it is difficult for an app to be truly \"mobile\" unless it is capable of still doing something useful where connectivity is lost. This is particularly the case for Defra family field workers who will often be working in remote, rural locations.\n\nTherefore, rather than providing purely \"mobile apps\", we will usually be providing what are commonly called **offline mobile apps** (a.k.a. **offline apps**).\n\nOffline apps can continue to be usable without an internet connection, including being able to work with data locally on the device **if** that is necessary for the app still to perform \"usefully\".\n\nWe should assume that any mobile apps we are providing are offline mobile apps unless we have a clear understanding, and written agreement with the users, that this is not a requirement, as this can often be a cause of confusion, and retrofitting offline functionality to a purely online mobile application after it has been provided may be very difficult or even not practical at all.\n\n### If only an online mobile app is needed its functionality should still degrade gracefully when offline\n\nEven if it is agreed that the app only needs to work online then it should still degrade gracefully when the connection is lost, for example by providing a clear message to the user that connection has been lost and the user should check their connection.\n\n## Which mobile platforms must be supported\n\n### Mobile applications for use within the Defra family must run on the Apple platform; apps intended for public use must run on at least Apple and Android devices\n\nMobile applications must run on mobile devices. Currently Defra has adopted Apple technology as the standard platform for mobile devices. **All mobile applications intended for internal use must therefore run on Apple devices. All mobile applications intended for by the general public, must run on at least Apple and Android devices. If a mobile application is intended for use outside Defra, but restricted to a specific and atypical (in terms of likely use of mobile technology as compared to the general public) user community, then we will need to be guided by the preferences of that user community in terms of the platform that we need to support.**\n\n## Troubleshooting\n\n### Before starting development you must have the knowledge, kit, privileges and app distribution arrangements in place to be able to remotely debug the app on suitable devices\n\nOften, issues will only be encountered when running apps on actual target devices. **Therefore you must have the knowledge, kit, privileges and app distribution arrangements in place to be able to remotely debug the app on suitable devices.**\n\n### The app must log errors and it must be possible for the user to make that information available to us for support purposes\n\nInformation about errors occurring on mobile devices will often be lost to us, making problem resolution much more difficult. To mitigate this issue **the app must log errors, and it must be possible for the user to make that information available to us for support purposes**, even if that is just via screenshot. It should also be possible to vary the level of detail logged, for example by implementing a \"debug\" mode. Early implementation of such features is very likely to also speed up error resolution during development itself.\n\n## Extra security considerations for mobile apps\n\n### Assess and address the security challenges to your app before finally agreeing app scope or minimal viable product\n\nData security is always essential, and mobile apps can be especially exposed to security risks and challenges. All mobile devices suffer from poor physical security in terms of being easily lost or stolen, and all have to operate over potentially insecure public networks. Furthermore, when an app is intended to run on a public device, then the security of the device is an unknown, and, by default, we should assume the device is inherently insecure. This means that the points below **must be considered and addressed very early in any project to develop such a mobile app**, as they could even influence the practical viability of the app or the scope of the requirements it can securely deliver. Design the complete security profile for your app before finally agreeing the scope of the app's requirements or the minimal viable product.\n\n### All traffic to and from the app must be encrypted\n\nAs mobile applications will usually be accessing the internet over entirely public networks **all traffic to and from the app must be encrypted**, for example via HTTPS/Transport Layer Security and/or via the use of a Virtual Private Network (VPN) and/or in-built application security wrapper. Please consult the latest security standards as to the appropriate level of encryption any such mechanisms should adopt, given the confidentiality marking of the data in question.\n\n### If the app needs to access Defra family systems that are non-public, developers and/or application architects must design the security profile for the app in consultation with the Cloud Mobile Services team in Group Infrastructure and Operations\n\n**If your app, which will normally be operating across the public internet, needs to access Defra family systems that are non-public, developers and/or application architects must consult with the Cloud Mobile Services team in Group Infrastructure and Operations**, who will advise on what is or isn't permissible and, where permissible, what mechanisms they can provide to allow such access to be done securely. For example, Cloud Mobile Services may advise as to VPN software and VPN end-points that they can make available to mobile devices that can allow safe access into \"back-end\", non-public Defra family systems in the cloud or on premise.\n\n### Also consider the guidance before designing the app's security profile\n\nThere are other recommendations to also bear in mind, especially around the security of data on the device and authentication strategies, so you are strongly advised to also read the accompanying **[guidance](../guides/mobile_app_guidance.md)** on mobile app development alongside these standards.\n\n## Which type of mobile technology to adopt\n\n### Decide which type of mobile technology to adopt by working through the options below in the stated order of preference, stopping as early as possible\n\nMobile apps will usually be required to work on multiple devices and operating systems. For example, for apps intended for general public use, we have already stated that mobile apps created by the Defra family must work on at least the Apple and Android platforms. That alone creates a considerable development challenge, and can mean continuing high costs of ownership, especially as those platforms are rapidly changing, and some suppliers, such as Apple, impose additional constraints and demands on users of their technology stack. Add in the need for the mobile app to work readily even with no internet access, and the additional concerns mobile poses to security, and the approach adopted to deliver a mobile app needs to be chosen extremely carefully, following the standard hierarchy explained below, **starting from the top and only working your way further down the options where it is clearly necessary to do so.**\n\n#### Option 0: Pick something \"off-the-shelf\"\n\nThis means that, **before developing an in-house app, it is even more important than for other bespoke developments to consider if a Commercial Off-The-Shelf (COTS) approach is preferred**. For example, is a suitable app already available in the app stores? Also, if the app is intended for use with an already adopted COTS suite, for example a time recording or asset management system, then is it possible that the supplier of that suite already has or will soon have a suitable app available, removing the need for us to develop our own app and then integrate that app with third party software? Then not only the development costs of the app, but also the ongoing costs of ownership, will be shared across the supplier's many customers rather than being having to be paid by us alone.\n\nIf using a third party app of any kind carefully assess whether use of that app will be appropriate given the sensitivity of the data it will be handling. In particular, many apps, especially free apps available in app stores, harvest user data, for example via Google Analytics. If those data harvesting features cannot be reliably disabled, and the data being processed by the app is in any way sensitive, then the use of such an app would be entirely inappropriate.\n\n#### Option 1: Develop a Microsoft Power App\n\nMicrosoft Power Apps offer a low or even no code way of developing mobile apps, which can then run within a Power Apps Mobile \"app player\" that comes in varieties downloadable to a range of mobile platforms. This option is likely to be appropriate if the app is going to have to interact with the rest of the Microsoft technology stack in the cloud, for example Dynamics or Office365, and when the app is simpler. Due to the Microsoft licensing model, it is also currently only likely to be appropriate for internal apps. Note that Microsoft frequently revises its licensing model, so you are advised to check out the current model before proceeding to assess what is possible, the costs and what licences we may already have available.\n\n#### Option 2: Develop a Progressive Web App (PWA)\n\n**Where a fully bespoke app is needed the next option to consider is a Progressive Web App (PWA), as recommended by the Government Digital Service here: <https://www.gov.uk/service-manual/technology/working-with-mobile-technology>**\n\nProgressive Web Apps provide cross platform and cross device support and can be delivered just like ordinary web apps to the browser, so no engagement with an app store or Mobile Device Management (MDM) software is needed, either by ourselves or the users, and the constraints of dealing with a proprietary platform, such as Apple, are avoided (for example, Apple require all apps to be re-released with a renewed provisioning profile at least once a year). Ongoing costs of ownership, as compared with other bespoke approaches, ought to be lower. Furthermore the skills involved in developing Progressive Web Apps are more transferable and much more likely to have a longer shelf life, so time and money invested in learning will have a bigger pay-off. This approach should also make it feasible to take existing web applications, and progressively enhance them until they can become PWAs.\n\nIt is possible to build PWAs that are fully functional offline mobile apps even on less standards compliant browsers such as Apple's Safari/WebKit. For more information on this please see read the \"Extra Guidance on Progressive Web Apps\" section in the accompanying **[guidance](../guides/mobile_app_guidance.md)**.\n\n#### Option 3: Build a cross-platform app\n\nIf, having considered above and done the recommended research, a PWA is not acceptable, the next option to consider is building the mobile app using a cross platform technology stack. These stacks are ways of using the same, or very close to the same, code-base but still building the app in ways that can allow it to run on multiple platforms and multiple devices. Otherwise a completely different code base, using a different language and development framework, is needed to build an app for each platform.\n\nThere are many such cross platform technology stacks, but some are obscure or are nearing obsolescence. This is still a too broad and fast moving area for us to as yet mandate solid standards, so you are therefore strongly advised to read the accompanying Mobile Application Guidance before considering a technology stack for developing cross platform apps (**[guidance](../guides/mobile_app_guidance.md)**).\n\nNote that cross platform app development, where in any way feasible, **is always preferable to native app development, even when we are currently targeting only a single platform (say Apple) and range of associated devices**. That is because such apps ought to be easily adaptable to run on a different platform, for example if our selected choice of devices changes, or if we decide to make the app, or a version of the app, available on more devices. Also, investment, in terms of time, money and \"hard lessons learned\", in cross platform app development is more likely to be applicable across the whole range of possible future apps than if we focus on a single native app technology. Furthermore, cross platform app development techniques also provide much better insulation against all the myriad changes that happen on the underlying platform itself (not completely, but better than with pure native development).\n\n### Don't build hybrid apps\n\nIn the past one common technique used to build cross platform apps was to actually run the app inside a hidden browser on the device, with such apps often being called \"hybrid\" apps. However, that is not the approach adopted by any of latest and more popular frameworks, and it is increasingly challenged by ever stricter adoption of \"single origin\" security restrictions in more modern browser engines. A mobile app rendering local content automatically uses up one \"origin\", to be precise, before it even starts to interact with any \\\"remote\\\" systems, plus single origin restrictions disallow delivering up local content via AJAX calls. Therefore, such frameworks including Apache Cordova, the closely related PhoneGap (which is Adobe's own Cordova variant) and Cordova-based Ionic (though Ionic also now has a React based version) must now be regarded as legacy and **not used for new projects**. It is still just about possible to work around such restrictions using the above frameworks, but the approaches then become painful and convoluted, for example Cordova-based Ionic does so by using a local web server built into the app. **Avoid these \"hybrid\" approaches for building a cross-platform app within a browser.**\n\nOf course, PWAs are also mobile apps that run within the browser, but with PWAs we are working \"with the grain\" of industry standards, that are becoming ever more capable all the time, where-as proprietary frameworks tend to eventually be allowed to fall behind browser standards, and then become gradually but inexorably less capable over time. For example, PWAs offer industry standards such as the indexedDB API to make managing and accessing local content much more straightforward than in any of the old \"hybrid\" app technologies. Also, because they are based on open W3C standards, knowledge gained building PWAs is likely to be useful for much longer and be more transferable.\n\n### Don't build native apps\n\nGiven all the alternative approaches, and the significant disadvantages of developing purely native apps, we do not currently anticipate any need to ever build native apps. Should this ever appear to be necessary, it will need to be managed as an exception to this standard.\n\n## Extra considerations for testing mobile apps\n\nThis section further supports the general testing guidance on the [Defra quality assurance and test wiki](https://github.com/DEFRA/qa-test/wiki).\n\n### Mobile apps must be tested on a representative range of platforms and devices\n\n**Mobile Apps must be tested on a representative range of platforms and devices.** That range of platforms and devices will obviously depend on the intended use of the app: for example, it can include the Apple platform and associated devices for an internal app, but for an app intended for use beyond the Defra family will include at least both the Apple and Android platforms and associated devices.\n\n### Clearly state the specific operating system versions and device types on which the app will be \"officially supported\"\n\n**At all times clearly state the specific operating system versions and device types on which the app will be tested, and where it is therefore \"officially supported\". This list will change constantly as operating systems and devices are updated.**\n\nAppropriate tools should be adopted to facilitate this. For example, if using a full functional IDE such as Xcode or Android Studio, make extensive use of the device simulators.\n\nHowever, no simulator is entirely perfect, and, once the mobile app has gone live, you will also be faced with the need to keep it tested and running despite rapid operating system updates (for example, for iPad OS these occur roughly monthly). It is therefore also strongly recommended that you acquire, and refresh as needed, a range of suitable devices for use in developing and then supporting the app.\n\n### For apps running on Defra managed devices, test the app on the currently latest Defra adopted version of iOS/iPad OS and the currently latest public version of iOS/iPad OS\n\n**If the app is to be deployed on Defra managed devices it must be tested on the currently latest Defra adopted version of iOS/iPad OS and the currently latest public version of iOS/iPad OS** (most of the time these versions will be the same, but there is normally a slight delay between Apple releasing a new version of iOS/iPad OS and Defra adoption, to ensure that no business critical apps have been negatively affected).\n\n### Test public apps on the appropriate versions of iOS/iPad OS and Android\n\n**If the app is intended for use outside Defra, then it must be tested on the versions of iOS/iPad OS stated above, plus it should also be tested on the previous public version, at least. It must also be tested on the latest version of Android and should also be tested on the previous version, at least.**\n\n### Use beta programmes to test apps on emerging versions of iOS/iPad OS or Android\n\n**Device/s must also be enrolled in any beta programmes and used to test emerging versions of iOS/iPad OS or Android, so that we have advance warning of any upcoming issues.**\n\n## Distributing and managing apps\n\n### Choose the mechanisms to distribute the app and app updates to end user devices before development starts\n\n**App delivery is impossible without a mechanism for distributing the app and continuing app updates to end user devices. It is therefore best to assess the app distribution options early in the project, in order to understand any implications for the project as quickly as possible, and to best ensure that testing during development can be done on a realistic basis.**\n\nFor progressive web apps this task is straight-forward: merely requiring the end user to navigate to the app's URL in a browser.\n\nDistribution of Android apps is very open. Android apps can be distributed via app stores, such as Google Play or the Amazon app store, via your own website or even sent via emails. Android apps could also, of course, be distributed internally via Defra Mobile Device Management software, currently AirWatch, but, as mobile applications intended for internal use must run on Apple devices, attempting to distribute Android apps this way would currently be pointless, as such apps cannot run on the mandated Apple devices.\n\nDistribution of Apple apps is, however, outside of distributions for purely development and testing purposes, very closed. Apple expect such apps to be distributed via the Apple app store, with the exception of apps that as an organisation intends to be used purely internally. Such internal apps can also, or instead, be distributed internally via an organisation's Mobile Device Management software (so in our case AirWatch) provided the organisation has purchased an Enterprise licence via the Apple Developer Enterprise Programme.\n\nThe Environment Agency is signed up to the Apple Developer Enterprise Programme, at least at the time of writing, May 2020, and can therefore distribute apps through the shared Defra family Mobile Device Management (MDM) software (i.e. AirWatch). As of the same date (May 2020) core Defra and the other parts of the Defra family are NOT signed up to the Apple Developer Enterprise Programme, meaning that they are contractually not meant to deploy their apps internally via our shared AirWatch. However, the costs of signing up are low, in the order of hundreds of pounds a year. **If you want to sign up another part of Defra to the Apple Enterprise Programme, first seek the advice of the Cloud Mobile Services team.**\n\n### Internal apps that can be managed by an internal mobile device management solution must be so managed (at least in production)\n\nDeploying an app intended for internal use via our MDM (currently AirWatch a.k.a. VMWare Workspace One) has many advantages in terms of app management and monitoring, such as, for example, controlling which user groups receive the app, allowing those groups to receive the app automatically or forcing the app to always use a provided VPN when connecting to specific URLs (this approach is called \"per app\" or \"in-app\" VPN). **Production distribution of internally developed apps intended purely for internal use, meaning use within the Defra family on managed devices, that can technically be managed via our MDM solution (so, for example, this standard cannot apply to PWAs) must therefore be via the Defra Mobile Device Management system, currently AirWatch. Note the unavoidable implication of this rule: this means that internal apps developed for use by any part of Defra other than the Environment Agency will require an Apple Developer Enterprise Programme membership to be arranged for the part of Defra that needs to use the app.**\n\n**The advantages of having apps, which we are using officially internally, managed via an MDM means that it will often (but not always) be desirable to also deploy third party apps via the same mechanism, in other words use our MDM (currently AirWatch a.k.a. VMWare Workspace One) to also manage these third party apps (in addition to apps we have developed internally).** Note, however, that as organisations are contractually only meant to use an MDM to distribute production apps to themselves, and then only when they have an Apple Developer Enterprise Programme licence, third party apps cannot be directly deployed to our MDM. Instead such apps must first be deployed to the app store. MDMs (including our current AirWatch set-up) then integrate via an Apple provided service called the Apple Business Manager **(<https://www.apple.com/uk/business/it/>** ) to the Apple app store, and are then able to manage apps pulled out of the app store and include them in the MDM's own application catalogues. (In AirWatch such apps can be managed in a very similar way to our own apps, such as being rolled out automatically to certain groups of users, or being forced to use a VPN).\n\nWe may also want a third party supplier to make, for example, an app developed or customised for our specific use available to us privately, rather than via the public part of the app store. Though, at the time of writing (May 2020), we have never used this facility, Apple have a service, called \"App Store Connect\", that is meant to allow private deployment from the Apple app store to a client's own MDM (<https://developer.apple.com/business/distribute/>).\n\n### No standards are mandated for the distribution of external apps, but there are technical and licensing restrictions to be aware of on the Apple platform\n\nThe above covers how we should manage apps for our own internal use (which, because of our standards about mobile devices, can currently only be Apple apps). We may also want to distribute apps externally.\n\nPublication of a public app to a well-known app store will often be desirable, for example to make the app more easily discoverable. However, bear in mind that distributing via an app store requires the app store providers to \"accept\" an app into their store and thus creates an extra \"hurdle\" to jump.\n\n**Therefore, no standards are mandated about how we should externally distribute the Android versions of internally developed apps.**\n\n**Unfortunately, though, Apple versions of internally developed apps can (and must) ONLY be externally distributed via the Apple app store.** It is technically possible to externally distribute an Apple app signed with an Enterprise licence by mechanisms other than the app store but, though this will work, Apple tend to regard it as a violation of licence terms and may summarily revoke the provisioning profile of the offending app, causing the app to immediately stop working.\n",
      "repository_path": "docs/standards/mobile_app_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:52.194000",
      "updated_at": "2025-01-24T11:12:52.194000"
    },
    {
      "_id": "679375b5dd8933f95f8d8f19",
      "text": "# Deployment standards\n\n## We use Docker containers for delivery of all bespoke software being deployed to AWS and Azure\nWhere we develop bespoke software, we default to using a Docker container as the deployable item.\n\nUnless the technology does not support containerisation, all deviations from this standard will be treated as architectural exceptions.\n\n### Rationale\nContainerisation is _most_ applicable to certain specific scenarios, such as highly-available applications with varying demand or when using technologies that have a frequent update cycle.\n\nHowever, running and managing a delivery pipeline and operational environment that supports _both_ containers and more traditional virtual machines results in a duplication of effort and technical solutions.\n\nTherefore, we will seek to minimise or eliminate this duplication by _always_ preferring containerisation.\n\nContainerisation also allows us to better manage a diverse application estate as it somewhat mitigates the need for across the board standardisation and large, coordinated upgrade projects, allowing instead for more nuanced decision-making.\n\n## We deploy code rather than containers to AWS Lambda and Azure Functions where their use is formally justified\nThere are cases where it will make sense to use a serverless platform rather than a containerised solution, but these _must_ be individually justified and recorded as formal architectural decisions.\n\nAdditionally, justifications for using serverless platforms can _only_ be considered for JavaScript or C# functions.\n\n### Rationale\nServerless platforms provide a greatly reduced operational burden, but are not so well represented in development and have certain limitations that can make their use inappropriate.\nThey also represent some vendor lock-in that, whilst minimal, is still not preferred.\n\n## We regularly review our use of serverless platforms\nIn particular, we will ensure that:\n- Vendor lock-in doesn't increase\n- We identify opportunities to rationalise these standards to use, effectively, \"serverless containers\"\n\n### Rationale\nAs serverless platforms are still relatively immature, we will need to ensure that our standards are still appropriate.\n",
      "repository_path": "docs/standards/deployment_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [
        "678fcbe9c24985192f15edc2",
        "678fcc2bc24985192f15edcc"
      ],
      "created_at": "2025-01-24T11:12:53.188000",
      "updated_at": "2025-01-24T11:12:53.188000"
    },
    {
      "_id": "679375b6dd8933f95f8d8f1a",
      "text": "# Version control standards\n\nThese standards define how version control is applied to our code.\n\nWherever possible these standards follow built-in versioning facilities for the languages, tools or frameworks we are using.\n\n## Rationale\n\n- Underpins a consistent, robust and repeatable release strategy\n- Ensures that we can have reliable dependency management between components even if they use different technologies\n\n## Standards\n\n### All code is held in a centrally managed Git repository\n\nThe primary repository is in the Defra GitHub organisation.\n\nAzure DevOps Git or hosted GitLab may be used to provide mirrors of GitHub repositories.\n\n### Every repository has a protected master branch with required status checks and approving reviews\n\nThis ensures that no change can be directly applied to production without appropriate test and review.\n\n### All releases are tagged in the version control system before deployment\n\nThis enables easy identification of the specific commits that form a release and a simple way for developers to checkout the code for a specific release for fault analysis purposes.\n\n### All releases have their version number included in the source code\n\nAll releases must include a commit that updates the version number (language dependent) in the source code. This ensures that the correct version can be identified in any published code.\n\n### Releases use semantic versioning\n\nApply [semantic versioning](https://semver.org/) when choosing a tag\n",
      "repository_path": "docs/standards/version_control_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:54.693000",
      "updated_at": "2025-01-24T11:12:54.693000"
    },
    {
      "_id": "679375b7dd8933f95f8d8f1b",
      "text": "# README standards\n\nEvery repo must have a README file in its root. The README is the starting point for anyone who wants to develop or test the repo. It provides an overview of what the repo is, and how to install, run and test its contents.\n\nThe README should include the following (if they apply):\n\n- **Description of the product** \u2013 what the service or product is, and what role this repo performs within it\n- **Prerequisites** \u2013 what you need to install or configure before you can set up the repo\n- **Setup process** - how to set up your local environment to work on the repo, including:\n  - development tools\n  - test tools\n- **How to run in development** \u2013 how to locally run the application in development mode after setup\n- **How to run tests** \u2013 how to run the test suite, broken into different categories if relevant (unit, integration, acceptance)\n- **Contributing to the project** - what to know before you submit your first pull request (this could also be in the form of a `CONTRIBUTING.md` file)\n- **Licence information** \u2013 what licence the repo uses (in addition to your `LICENSE` file)\n\nThe file should be in Markdown format (.md).\n\n## Additional detail\n\nYou may also want to include:\n\n- How the product fits into wider architecture\n- Internal product architecture\n- Data structure\n- API end points\n- Monitoring\n- Error handling\n- Audit\n- User access\n- Security\n- Complexity worth documenting\n- Pipelines\n\nIf this documentation is too lengthy or complex, it doesn't have to be in the README. Just make sure your README tells users where to find these additional docs.\n\n## A basic README.md template\n\n```markdown\n# name-of-repo\n\nThe \"Register your dinosaur\" service allows customers to apply online for a dinosaur licence.\n\nThis application handles the backend dinosaur processing.\n\n##\u00a0Prerequisites\n\n## Setup\n\n### Development\n\n### Test\n\n## Running in development\n\n## Running tests\n\n## Contributing to this project\n\nPlease read the [contribution guidelines](/CONTRIBUTING.md) before submitting a pull request.\n\n## Licence\n\nTHIS INFORMATION IS LICENSED UNDER THE CONDITIONS OF THE OPEN GOVERNMENT LICENCE found at:\n\n<http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3>\n\nThe following attribution statement MUST be cited in your products and applications when using this information.\n\n>Contains public sector information licensed under the Open Government licence v3\n\n### About the licence\n\nThe Open Government Licence (OGL) was developed by the Controller of Her Majesty's Stationery Office (HMSO) to enable information providers in the public sector to license the use and re-use of their information under a common open licence.\n\nIt is designed to encourage use and re-use of information freely and flexibly, with only a few conditions.\n```\n",
      "repository_path": "docs/standards/readme_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:55.841000",
      "updated_at": "2025-01-24T11:12:55.841000"
    },
    {
      "_id": "679375b9dd8933f95f8d8f1c",
      "text": "# TSQL and SQL Server database standards\n\n## Naming\n\n### Tables\n\nRules: Pascal notation; end with an `s`\n\nExamples: `Products`, `Customers`. Group related table names<sup>1</sup>\n\n### Stored Procs\n\nRules: sp[App Name]\\_[Group Name]_[Action][table/logical instance]\n\nExamples: `spOrders_GetNewOrders`, `spProducts_UpdateProduct`\n\n### Triggers\n\nRules: TR_[TableName]_[action]\n\nExamples: `TR_Orders_UpdateProducts`\n\n*Note. The use of triggers is discouraged*\n\n### Indexes\n\nRules: IX_[TableName]_[column]ID\n\nExamples: `IX_Products_ProductID`\n\n### Primary Keys\n\nRules: PK_[TableName]\n\nExamples: `PK_Products`\n\n### Foreign Keys\n\nRules: FK_[TableName1]_[TableName2]\n\nExample: `FK_Products_Orders`\n\n### Defaults\n\nRules: DF_[TableName]_[ColumnName]\n\nExample: `DF_Products_Quantity`\n\n### Columns\n\nIf a column references another table\u2019s column, name it [table name]ID\n\nExample: The Customers table has an ID column. The Orders table should have a `CustomerID` column\n\n## General Rules\n\n- Do not use spaces in the name of database objects\n- Do not use SQL keywords as the name of database objects. In cases where this is necessary, surround the object name with brackets, such as `[Year]`\n- Do not prefix stored procedures with `sp_`<sup>2</sup>\n- Prefix table names with the owner name<sup>3</sup>\n\n## Structure\n\n- Each table must have a primary key\n\t- In most cases it should be an `IDENTITY` column named ID\n- Normalize data to third normal form\n\t- Do not compromise on performance to reach third normal form. Sometimes, a little denormalization results in better performance.\n- Do not use `TEXT` as a data type; use the maximum allowed characters of `VARCHAR` instead\n- In `VARCHAR` data columns, do not default to `NULL`; use an empty string instead\n- Columns with default values should not allow NULLs\n- As much as possible, create stored procedures on the same database as the main tables they will be accessing\n\n## Formatting\n\n- Use upper case for all SQL keywords\n\t- `SELECT`, `INSERT`, `UPDATE`, `WHERE`, `AND`, `OR`, `LIKE`, etc.\n- Indent code to improve readability\n- Comment code blocks that are not easily understandable\n\t- Use single-line comment markers(`--`)\n\t- Reserve multi-line comments (`/*.. ..*/`) for blocking out sections of code\n- Use single quote characters to delimit strings.\n\t- Nest single quotes to express a single quote or apostrophe within a string\n\t\t- For example, `SET @sExample = 'SQL''s Authority'`\n- Use parentheses to increase readability\n\t- `WHERE (color=\u2019red\u2019 AND (size = 1 OR size = 2))`\n- Use `BEGIN..END` blocks only when multiple statements are present within a conditional code segment.\n- Use one blank line to separate code sections.\n- Use spaces so that expressions read like sentences.\n\t- `fillfactor = 25`, not `fillfactor=25`\n- Format `JOIN` operations using indents\n\t- Also, use ANSI Joins instead of old style joins<sup>4</sup>\n- Place `SET` statements before any executing code in the procedure.\n\n## Coding\n\n- Optimize queries using the tools provided by SQL Server<sup>5</sup>\n- Do not use `SELECT *`\n- Return multiple result sets from one stored procedure to avoid trips from the application server to SQL server\n- Avoid unnecessary use of temporary tables\n\t- Use 'Derived tables' or CTE (Common Table Expressions) wherever possible, as they perform better<sup>6</sup>\n- Avoid using `<>` as a comparison operator where possible\n\t- Consider using `ID IN(1,3,4,5)` instead of `ID <> 2`\n- Use `SET NOCOUNT ON` at the beginning of stored procedures<sup>7</sup>\n- Do not use cursors or application loops to do inserts<sup>8</sup>\n\t- Instead, use `INSERT INTO`\n- Fully qualify tables and column names in JOINs\n- Fully qualify all stored procedure and table references in stored procedures.\n- Consider not defining default values for parameters.\n\t- If a default is needed, the front end should supply the value where possible.\n- Place all `DECLARE` statements before any other code in the procedure.\n- Do not use column numbers in the `ORDER BY` clause.\n- Do not use `GOTO`.\n- Check the global variable `@@ERROR` immediately after executing a data manipulation statement (like `INSERT`/`UPDATE`/`DELETE`), so that you can rollback the transaction if an error occurs\n\t- Or use `TRY`/`CATCH`\n- Do basic validations in the front-end itself during data entry\n- Off-load tasks, like string manipulations, concatenations, row numbering, case conversions, type conversions etc., to the front-end applications if these operations are going to consume more CPU cycles on the database server\n- Always use a column list in your `INSERT` statements.\n\t- This helps avoid problems when the table structure changes (like adding or dropping a column).\n- Minimize the use of NULLs, as they often confuse front-end applications, unless the applications are coded intelligently to eliminate NULLs or convert the NULLs into some other form.\n\t- Any expression that deals with `NULL` results in a `NULL` output.\n\t- The `ISNULL` and `COALESCE` functions are helpful in dealing with `NULL` values.\n- Do not use the identitycol or rowguidcol.\n- Avoid the use of cross joins, if possible.\n- When executing an `UPDATE` or `DELETE` statement, use the primary key in the `WHERE` condition, if possible. This reduces error possibilities.\n- Avoid using `TEXT` or `NTEXT` datatypes for storing large textual data.<sup>9</sup>\n\t- Use the maximum allowed characters of `VARCHAR` instead\n- Avoid dynamic SQL statements as much as possible.<sup>10</sup>\n- Access tables in the same order in your stored procedures and triggers consistently.<sup>11</sup>\n- Do not call functions repeatedly within your stored procedures, triggers, functions and batches.<sup>12</sup>\n- Default constraints must be defined at the column level.\n- Avoid wild-card characters at the beginning of a word while searching using the `LIKE` keyword, as these results in an index scan, which defeats the purpose of an index.\n- Define all constraints, other than defaults, at the table level.\n- When a result set is not needed, use syntax that does not return a result set.<sup>13</sup>\n- Avoid rules, database level defaults that must be bound or user-defined data types. While these are legitimate database constructs, opt for constraints and column defaults to hold the database consistent for development and conversion coding.\n- Constraints that apply to more than one column must be defined at the table level.\n- Where the `CHAR` data type is appropriate for a column. Only use it when the column is non-nullable.<sup>14</sup>\n- Do not use white space in identifiers.\n- The `RETURN` statement is meant for returning the execution status only, but not data.\n\n## Reference\n\n1. Group related table names:\n\n- `Products_UK`\n- `Products_India`\n- `Products_Mexico`\n\n2. The prefix sp_ is reserved for system stored procedures that ship with SQL Server. Whenever SQL Server encounters a procedure name starting with sp_, it first tries to locate the procedure in the master database, then it looks for any qualifiers (database, owner) provided, then it tries dbo as the owner. Time spent locating the stored procedure can be saved by avoiding the `sp_` prefix.\n\n3. This improves readability and avoids unnecessary confusion. Microsoft SQL Server Books Online states that qualifying table names with owner names helps in execution plan reuse, further boosting performance.\n\n4. Example of 'False code'\n\n```sql\nSELECT *\nFROM Table1, Table2\nWHERE Table1.d = Table2.c\n```\n\nExample of 'True code'\n\n```sql\nSELECT *\nFROM Table1\nINNER JOIN Table2 ON Table1.d = Table2.c\n```\n\n5. Use the graphical execution plan in Query Analyzer or `SHOWPLAN_TEXT` or `SHOWPLAN_ALL` commands to analyze your queries. Make sure your queries do an 'Index seek' instead of an 'Index scan' or a 'Table scan.' A table scan or an index scan is a highly undesirable and should be avoided where possible.\n\n6. Consider the following query to find the second highest offer price from the `Items` table\n\n```sql\nSELECT MIN(Price)\nFROM Products\nWHERE ID IN\n(\nSELECT TOP 2 ID\nFROM Products\nORDER BY Price Desc\n)\n```\n\nThe same query can be re-written using a derived table, as shown below, and it performs generally twice as fast as the above query.\n\n```sql\nSELECT MIN(Price)\nFROM\n(\nSELECT TOP 2 Price\nFROM Products\nORDER BY Price DESC\n)\n```\n\n7. This suppresses messages like `(1 row(s) affected)` after executing `INSERT`, `UPDATE`, `DELETE` and `SELECT` statements. Performance is improved due to the reduction of network traffic.\n\n8. Try to avoid server side cursors as much as possible. Always stick to a 'set-based approach' instead of a 'procedural approach' for accessing and manipulating data. Cursors can often be avoided by using `SELECT` statements instead. If a cursor is unavoidable, use a `WHILE` loop instead. For a `WHILE` loop to replace a cursor, however, you need a column (primary key or unique key) to identify each row uniquely.\n\n9. You cannot directly write or update text data using the `INSERT` or `UPDATE` statements. Instead, you have to use special statements like `READTEXT`, `WRITETEXT` and `UPDATETEXT`. So, if you don't have to store more than 8KB of text, use the `CHAR(8000)` or `VARCHAR(8000)` datatype instead.\n\n10. Dynamic SQL tends to be slower than static SQL, as SQL Server must generate an execution plan at runtime. `IF` and `CASE` statements come in handy to avoid dynamic SQL.\n\n11. This helps to avoid deadlocks. Other things to keep in mind to avoid deadlocks are:\n\n  - Keep transactions as short as possible.\n  - Touch the minimum amount of data possible during a transaction.\n  - Never wait for user input in the middle of a transaction.\n  - Do not use higher level locking hints or restrictive isolation levels unless they are absolutely needed.\n\n12. You might need the length of a string variable in many places of your procedure, but don't call the `LEN` function whenever it's needed. Instead, call the `LEN` function once and store the result in a variable for later use.\n\n13. Resultset syntax\n\n```sql\nIF EXISTS (SELECT 1 FROM Products WHERE ID = 50)\n```\n\nInstead of\n\n```sql\nIF EXISTS (SELECT COUNT(ID) FROM Products WHERE ID = 50)\n```\n\n14. `CHAR(100)`, when `NULL`, will consume 100 bytes, resulting in space wastage. Preferably, use `VARCHAR(100)` in this situation. Variable-length columns have very little processing overhead compared with fixed-length columns.\n",
      "repository_path": "docs/standards/tsql_and_sqldb_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:12:57.627000",
      "updated_at": "2025-01-24T11:12:57.627000"
    },
    {
      "_id": "679375bbdd8933f95f8d8f1d",
      "text": "# Development languages\n\nThis standard defines the development languages we use.\n\nThe standard has come from delivering digital services that tend to follow this pattern:\n- A bespoke online digital service that is specifically built to meet well-researched user needs\n- One or more common Defra core IT systems that support our business\n- Pursuing a Cloud first agenda and giving equal consideration to open source software\n \n## We use more than one development language\nWe recognise the value of choosing the right tool for the job so we do not specify one single development language\nfor everything we deliver.\n\nThe advantages of this approach are:\n- it increases our ability to rapidly respond to changing business needs\n- we can more easily adopt new developments in the IT industry\n- vendor and product selection is less constrained\n- it strengthens our supplier management capability\n- reduced development cost\n- better opportunities for re-use of external code, libraries, products and best practice\n- happier developers!\n\nThere are some disadvantages, such as the following, but these are out-weighed by the advantages:\n- staff recruitment, retention and work allocation is more difficult\n- there are fewer opportunities to share internal code and best practice\n- it can be difficult to maintain enough capability with only a small number of developers\n- it adds extra complexity to technology choices\n \n## We build on open platforms whenever possible\nThe government\n[Technology Code of Practice](https://www.gov.uk/government/publications/technology-code-of-practice/technology-code-of-practice)\nstates that you must:\n- Be open and use open source\n- Make use of open standards\n\nThe simplest way to achieve this is by using open platforms by default.\n \n## Our primary development platforms are Microsoft's .NET and Node.js\nThere are two key areas within Defra where we need to write custom code.\n\nWe build bespoke online digital services using standard government libraries and frameworks.\nFor these we have identified Node.js as the most suitable platform.\n\nWe also build solutions on top of commercial software products, where we have to use the platform that the vendor\nprovides.\nMany of those products are from Microsoft, which means that we use .NET for this.\n\nThese two platforms provide a wide range of capabilities so we can use them for most of the applications we\ndevelop.  We benefit from the flexibility of having two distinct platforms to choose from but also limit the\ndiversity to a level that is appropriate for a medium-sized team of developers\n\nHowever, these two technologies alone will not _always_ be the solution to every problem and we also have a range of\nexisting applications within Defra that are built using other platforms. So, we still need to maintain some limited\ncapability in many other technologies such as Java, Python, R, Ruby and VBA.\n \n## We develop online digital services in Node.js\nThe world of online digital development is dominated by open source technologies so there are many freely available\nframeworks, libraries and tools to support it.\n\nThese kind of services change rapidly, which has led to the rise of rapid development frameworks and a preference for\ndynamic languages.\n\nBuilding for the Web means that developers already need a knowledge of JavaScript, so using Node.js eases the\nburden on developers to maintain capability in multiple languages. Also, Node.js has proven to deliver good\napplication performance in most Web-facing scenarios.\n\nThis can be seen across government as well, in particular the GDS front end community tend to publish their products\nso that they can be easily used in Ruby and Node.js, in particular.\n \n## We use JavaScript as our Node.js development language\nWe do not use TypeScript (or any other extensions to the JavaScript language). \n\nFor the types of digital services we build, TypeScript is unneccessary as any benefits don't outweigh the overhead. \nUse vanilla JavaScript.\n\n## We use the Hapi framework to develop in Node.js\nHapi is already in use in Defra digital services and has provided a productive level of standardisation across\ndevelopment teams and has proven to be robust and reliable.\n \n## We minimise the customisation inside commodity platforms\nOur enterprise architecture is based around commodity platforms, but these often require customisation\nto meet our needs.\n\nThis customisation leads to a maintainability burden, so we prefer to deliver custom capability _outside_ of the\nplatform itself. To do this, we build bespoke functionality but provide a mechanism to include that functionality\nin the user interface of the product. Most of the products we use have some way of doing this built in.\n\nTaking this approach means that we decouple the bespoke capability from the core product, making it easier to perform\nchanges and upgrades, but we also provide a coherent user interface.\n \n## We use .NET to customise and extend commodity products\nMany of our commodity products come from Microsoft and so provide a .NET customisation platform.\n\nIn addition, they will often provide highly capable .NET libraries and SDKs that enable integration with the products,\nincluding add-in functionality such as SharePoint Provider Hosted Add-Ins.\n \n## We use C# as our .NET development language\nC# is the de facto standard language for .NET development and owing to its similarity to Java provides a level of\nfamiliarity and cross-fertilisation with other language communities.\n\nIt is also the language that is best supported in open source .NET managed frameworks.\n \n## We use .NET Core wherever practical in preference to .NET Framework\n.NET Core is Microsoft's strategic direction for .NET and gives all the benefits of being open source and cross platform,\nincluding growing community support.\n",
      "repository_path": "docs/standards/development_language_standards.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [
        "678fcbe9c24985192f15edc2",
        "6790cb8170024c99a94725a5",
        "678fcc01c24985192f15edc6",
        "678fcc2bc24985192f15edcc"
      ],
      "created_at": "2025-01-24T11:12:59.338000",
      "updated_at": "2025-01-24T11:12:59.338000"
    },
    {
      "_id": "679375bcdd8933f95f8d8f1e",
      "text": "---\nname: Bug report\nabout: Found a spelling mistake or broken link? Let us know\ntitle: ''\nlabels: bug\nassignees: ''\n\n---\n\n**Describe the bug**\nA clear and concise description of what the issue is. Ex. Principles link on README page is broken\n\n**Expected behavior**\nA clear and concise description of what you expected to happen or see. Ex. The correct spelling is 'Principle'.\n\n**Additional context**\nAdd any other relevant context about the issue here.\n",
      "repository_path": ".github/ISSUE_TEMPLATE/bug_report.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:13:00.595000",
      "updated_at": "2025-01-24T11:13:00.595000"
    },
    {
      "_id": "679375bedd8933f95f8d8f1f",
      "text": "---\nname: Suggest an idea\nabout: Suggest an idea for this project, for example a new standard\ntitle: ''\nlabels: enhancement\nassignees: ''\n\n---\n\n**Is your suggestion related to a problem? Please describe.**\nA description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\nIf you have a solution try and provide a description of what you want to happen.\n\n**Describe alternatives you've considered**\nA description of any alternative solutions or ideas you've considered.\n\n**Additional context**\nAdd any other context about the suggestion here.\n",
      "repository_path": ".github/ISSUE_TEMPLATE/suggest-an-idea.md",
      "standard_set_id": "67937583e3543492a2876154",
      "classification_ids": [],
      "created_at": "2025-01-24T11:13:02.074000",
      "updated_at": "2025-01-24T11:13:02.074000"
    }
  ],
  "classifications": [
    {
      "_id": "678fcbe9c24985192f15edc2",
      "name": "Javascript",
      "created_at": "2025-01-21T16:31:37.157000",
      "updated_at": "2025-01-21T16:31:37.157000"
    },
    {
      "_id": "678fcbf5c24985192f15edc4",
      "name": "Python",
      "created_at": "2025-01-21T16:31:49.242000",
      "updated_at": "2025-01-21T16:31:49.242000"
    },
    {
      "_id": "678fcc01c24985192f15edc6",
      "name": ".Net",
      "created_at": "2025-01-21T16:32:01.068000",
      "updated_at": "2025-01-21T16:32:01.068000"
    },
    {
      "_id": "678fcc2bc24985192f15edcc",
      "name": "C#",
      "created_at": "2025-01-21T16:32:43.557000",
      "updated_at": "2025-01-21T16:32:43.557000"
    },
    {
      "_id": "6790cb8170024c99a94725a5",
      "name": "Node.js",
      "created_at": "2025-01-22T10:42:09.759000",
      "updated_at": "2025-01-22T10:42:09.759000"
    },
    {
      "_id": "67935eda361ba58e4bb051aa",
      "name": "Java",
      "created_at": "2025-01-24T09:35:22.049000",
      "updated_at": "2025-01-24T09:35:22.049000"
    }
  ],
  "code_reviews": [],
  "standard_sets": [
    {
      "_id": "6793749ca3cf6891cb41f3e0",
      "name": "Test Standards",
      "repository_url": "https://github.com/ee-todd/test-standards-set",
      "custom_prompt": "Custom Prompt here",
      "created_at": "2025-01-24T11:08:12.057000",
      "updated_at": "2025-01-24T11:08:12.057000"
    },
    {
      "_id": "67937583e3543492a2876154",
      "name": "Defra Software Development Standards",
      "repository_url": "https://github.com/DEFRA/software-development-standards",
      "custom_prompt": "Custom Prompt here",
      "created_at": "2025-01-24T11:12:03.675000",
      "updated_at": "2025-01-24T11:12:03.675000"
    }
  ]
}